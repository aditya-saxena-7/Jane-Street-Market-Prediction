# -*- coding: utf-8 -*-
"""JaneStreetSubmission.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13Sag1ZbW2x01x0_I6bkctErnPIzSlJCr

<h1><center> Jane Street Market Prediction Solution Notebook by Aditya Saxena</h1>

![](https://storage.googleapis.com/kaggle-organizations/3761/thumbnail.png?r=38)

<h2> Table of content </h2>
<ul> <li> <a href="#preparation"> Preparation </a> </li>
    <li> <a href="#load_data"> Load The Datas </a> </li>
    <li> <a href="#first_look"> First Look at The Data </a> </li>
    <li> <a href="#eda"> Explorations Data analysis </a> </li>
    <li> <a href="missing_values"> Missing Values </a> </li>
    <li> <a href="feature_engineering"> Feature engineering </a> </li>
    <li> <a href="feat_exploration"> Features dataset explorations </a> </li>
    <li> <a href="modeling"> Modeling </a> </li>
    <li> <a href="submission"> Submission </a> </li>
</ul>
<hr>

<h2 id=preparation> Preparation </h2>

The provided code snippet sets up the initial environment and imports necessary libraries for the Jane Street Market Prediction Challenge. Here’s a brief explanation of each part:

1. **Libraries Import**:
   - `numpy` and `pandas`: Essential for data manipulation and numerical operations.
   - `seaborn`, `matplotlib.pyplot`, `plotly.express`: Used for data visualization.
   - `sklearn` libraries: Include tools for machine learning, such as `LinearRegression`, `TSNE`, `KMeans`, and evaluation metrics like `roc_auc_score`.
   - `lightgbm`: A gradient boosting framework for efficient and scalable training.
   - `tqdm`: Provides progress bars for loops.
   - `pickle`: For saving and loading model objects.
   - `os`: For file and directory management.
   - `gc`: For garbage collection to manage memory.

2. **File Directory Walk**:
   - The `os.walk` function iterates through the files in the '/kaggle/input' directory, printing out the paths. This helps in identifying and loading datasets.

3. **Data Handling and Model Training**:
   - `train_test_split`: Splits the dataset into training and testing subsets.
   - `KFold`: Used for cross-validation, which helps in assessing the model's performance on different subsets of the data.
   - The inclusion of visualization and clustering techniques (`tsne`, `KMeans`) helps in understanding data structure and relationships.

### Importance:
- **Library Imports**: Essential for different stages of data analysis and model building.
- **Data Visualization**: Helps in exploring and understanding the data, identifying patterns and anomalies.
- **Machine Learning Tools**: Libraries like `sklearn` and `lightgbm` provide the functionality needed to build and evaluate predictive models.
- **Cross-Validation**: Ensures the model generalizes well to unseen data, improving its robustness.

Overall, this setup prepares the environment for data exploration, preprocessing, modeling, and evaluation, crucial steps in the machine learning pipeline.
"""

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import seaborn as sns
import matplotlib.pyplot as plt
import plotly.express as px
import matplotlib.gridspec as gridspec
from  collections import defaultdict
from sklearn.linear_model import LinearRegression
from sklearn.manifold import TSNE as tsne
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
import os
import gc
import pickle
import lightgbm as lgbm
from sklearn.model_selection import train_test_split
from tqdm import tqdm
from sklearn.metrics import roc_curve,auc,roc_auc_score
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))
from sklearn.model_selection import KFold

"""### Explanation of the Function

The function `save_pickle` saves a Python dictionary to a file using the `pickle` module. Here's a breakdown of the function:

1. **Function Definition**:
   - `def save_pickle(dict_param, nom_fich):`
     - **`dict_param`**: The dictionary you want to save.
     - **`nom_fich`**: The name of the file where the dictionary will be saved.

2. **Opening the File**:
   - `with open(nom_fich, "wb") as f:`
     - Opens the file `nom_fich` in write-binary mode (`"wb"`). The `with` statement ensures the file is properly closed after the block of code is executed.

3. **Saving the Dictionary**:
   - `pickle.dump(dict_param, f)`
     - Uses `pickle.dump` to serialize `dict_param` and write it to the file `f`.

### Importance in Machine Learning Pipeline

- **Model Persistence**: Saving models, parameters, or other data objects allows for easy retrieval and reuse without needing to re-compute them. This is crucial for long-running tasks or when sharing models with others.
- **Efficiency**: Pickling is efficient for saving and loading large Python objects, which can save time and computational resources.
- **Reproducibility**: By saving the exact state of the model or parameters, you can ensure that your results are reproducible.

### Terminologies

- **Pickle**: A Python module that serializes (converts to a byte stream) and deserializes (reconstructs from a byte stream) Python objects. It's used for saving complex data types, like dictionaries or models.
- **Serialization**: The process of converting an object into a format that can be easily saved to a file or transmitted and then reconstructed later.

### Example Usage

If you have a dictionary of model parameters:

```python
model_params = {"learning_rate": 0.01, "n_estimators": 100, "max_depth": 5}
save_pickle(model_params, "model_params.pkl")
```

This code will save `model_params` to a file named `model_params.pkl`, which can be loaded later for reuse.
"""

def save_pickle(dict_param,nom_fich):
    with open(nom_fich,"wb") as f :
        pickle.dump(dict_param,f)

"""<h2 id=load_data> Load The Datas"""

# The root path
path = "/kaggle/input/jane-street-market-prediction/"

# Load the training datas.
train = pd.read_csv(path + "train.csv")

# Load the metadata pertaining to the anonymized features.
features = pd.read_csv(path + "features.csv")

"""<h2 id=first_look> First Look at the Data </h2>

### Explanation of the Data and File Structure

#### Training Data Overview

The training dataset contains the following columns:
- **date**: An integer representing the day of the trade.
- **weight**: The weight associated with each trade, impacting its importance in the evaluation.
- **resp, resp_1, resp_2, resp_3, resp_4**: These represent returns over different time horizons. `resp` is the primary return, while `resp_1` to `resp_4` represent returns over shorter or longer periods.
- **feature_0 to feature_129**: Anonymized features representing various market indicators and data points for each trading opportunity.
- **ts_id**: Time series identifier for maintaining the order of trades.

#### Dataset Description

- **Anonymized Features**: The dataset has 130 features (from `feature_0` to `feature_129`), which are anonymized but represent real stock market data.
- **Predictive Task**: For each row (trading opportunity), the goal is to predict an action (`1` to make the trade, `0` to pass).
- **Returns and Weights**: Each trade has an associated return (`resp`) and weight, where trades with `weight = 0` are included for completeness but do not contribute to the evaluation score.

#### Files Provided

- **train.csv**: Contains historical trading data, including features and returns.
- **example_test.csv**: A mock test set to understand the structure of the unseen test set.
- **example_sample_submission.csv**: A sample submission file demonstrating the correct format for submissions.
- **features.csv**: Metadata related to the anonymized features.

#### Competition Structure

- **Time-Series API**: Ensures that models do not peek forward in time, maintaining the integrity of the time-series nature of the data.
- **Evaluation Phases**:
  - **Model Training Phase**: Uses approximately 1 million rows of historical data for training.
  - **Live Forecasting Phase**: Uses periodically updated live market data for testing.

### Importance in Machine Learning Pipeline

1. **Data Understanding**: Knowing the structure and content of the dataset is crucial for effective data preprocessing and feature engineering.
2. **Time-Series Considerations**: Maintaining the order of trades (time-series) is vital to ensure realistic and robust model predictions.
3. **Evaluation and Submission**: Understanding how to properly submit predictions using the provided API and the importance of the return values in scoring.

This step is fundamental in setting up the entire machine learning pipeline, ensuring data integrity, and preparing for accurate model training and evaluation.
"""

# Take a look at the training data
train.head()

# info about training datas.
train.info()

"""### Statistical Overview of Training Data

Here’s a summary of the statistical analysis from the `train.describe()` output for the Jane Street Market Prediction Challenge dataset:

#### General Information
- **Number of rows**: Approximately 2.39 million (`2.390491e+06`), indicating a substantial dataset suitable for robust model training and testing.

#### Key Columns and Their Statistics
- **date**: Ranges from `0` to `499`, representing different trading days.
- **weight**: Mean weight of trades is `3.03` with a standard deviation of `7.67`. This indicates most weights are concentrated around the mean but can vary significantly.
- **resp, resp_1, resp_2, resp_3, resp_4**: These represent returns over different time horizons. Mean returns are close to zero, indicating balanced gains and losses across the dataset. The standard deviations suggest some volatility.

#### Features (feature_0 to feature_129)
- **feature_0**: Binary feature with values `-1` and `1`, possibly indicating some categorical property.
- **feature_1 to feature_129**: These features have varying ranges and distributions. The standard deviations, means, and min/max values indicate they capture diverse aspects of the trading data, crucial for model prediction.

### Importance of Statistics in Machine Learning Pipeline

- **Data Understanding**: Statistical summaries help understand the distribution, range, and central tendencies of the data. This is crucial for identifying patterns, anomalies, and potential preprocessing steps.
- **Feature Engineering**: Insights from descriptive statistics guide feature transformations, normalizations, and the handling of outliers, which can improve model performance.
- **Model Training and Validation**: Understanding the target variable (`resp` and related returns) distributions aids in evaluating model performance, setting realistic expectations for returns, and designing appropriate evaluation metrics.

### Terminologies Explained

- **Mean**: Average value of the data column.
- **Standard Deviation (std)**: Measures the dispersion or spread of the data points from the mean.
- **Min and Max**: The minimum and maximum values in the column, indicating the range.
- **Percentiles (25%, 50%, 75%)**: Values below which a certain percentage of data falls. For instance, the 50th percentile (median) is the value below which 50% of the data lies.

These statistics are crucial steps in the data exploration phase, enabling informed decisions throughout the machine learning pipeline.
"""

# some statistics on the training datas.
train.describe()

"""### Display Missing Values per Column

The provided code calculates and displays the percentage of missing values for each column in the training dataset.

```python
missing_table = pd.DataFrame({c: (train[c].isna().sum() / len(train)) * 100 for c in train.columns}, index=["% missing values"])
missing_table
```

#### Explanation of the Code

1. **Creating Missing Values Table**:
   - The dictionary comprehension `{c: (train[c].isna().sum() / len(train)) * 100 for c in train.columns}` iterates over each column `c` in the training dataset (`train.columns`).
   - For each column, it calculates the percentage of missing values:
     - `train[c].isna().sum()`: Counts the number of missing values in column `c`.
     - `len(train)`: Gets the total number of rows in the dataset.
     - `(train[c].isna().sum() / len(train)) * 100`: Computes the percentage of missing values.
   - The resulting dictionary is then used to create a DataFrame with an index labeled `"% missing values"`.

2. **Displaying the Missing Values Table**:
   - The resulting DataFrame `missing_table` is displayed, showing the percentage of missing values for each column.

#### Interpretation of the Missing Values Table

Here’s a brief summary based on the provided output:

- **Columns with Missing Values**:
  - Some features (e.g., `feature_121`) have a significant percentage of missing values (up to 2.92%).
  - Other features (e.g., `feature_122`, `feature_123`) have minimal missing values (~0.009%).
  - Most columns, including important ones like `date`, `weight`, `resp`, and `ts_id`, have no missing values.

#### Importance of Handling Missing Values

- **Data Quality**: Missing values can affect the quality and performance of machine learning models. Identifying and handling them is crucial.
- **Imputation Strategies**: Depending on the percentage of missing values, different strategies can be employed:
  - For low percentages, imputation with mean, median, or mode might be effective.
  - For higher percentages, more sophisticated techniques or even dropping the columns might be considered.
- **Model Robustness**: Properly handling missing values ensures that the model is trained on clean data, leading to more reliable and robust predictions.

This step is vital in the data preprocessing phase of the machine learning pipeline, ensuring the data is clean and ready for model training.
"""

# Display missing values per column

missing_table = pd.DataFrame({c:(train[c].isna().sum()/len(train))*100 for c in train.columns},index=["% missing values"])

missing_table

"""===> It's obvious , that we should treat effeciently the little number of missing values in the column from the feature_120 to the feature_129.

<h2 id=eda> Explorations Data analysis </h2>

### Explanation of the Code: Displaying Histograms of Features

The provided code creates histograms for all the features in the training dataset using the `seaborn` library. Here’s a detailed breakdown:

#### Code Breakdown

1. **Setting Up the Plot Grid**:
   ```python
   fig, axes = plt.subplots(nrows=45, ncols=3, figsize=(25, 250))
   ```
   - **fig, axes**: `fig` is the figure object, and `axes` is a 2D array of subplots.
   - **nrows=45, ncols=3**: Specifies a grid of 45 rows and 3 columns, creating 135 subplots.
   - **figsize=(25, 250)**: Sets the size of the entire figure to be very large to accommodate all subplots.

2. **Looping Through Features**:
   ```python
   for i in range(2, 137):
       sns.distplot(train.iloc[:, i], ax=axes[(i-2) // 3, (i-2) % 3])
   ```
   - **range(2, 137)**: Loops from the third column (index 2) to the 137th column (index 136). This skips the first two columns (`date` and `weight`).
   - **train.iloc[:, i]**: Selects the `i-th` column of the `train` DataFrame.
   - **sns.distplot**: Plots the distribution of the selected column.
   - **ax=axes[(i-2) // 3, (i-2) % 3]**: Specifies the subplot in which to plot the histogram. The formula `(i-2) // 3` calculates the row index and `(i-2) % 3` calculates the column index for the subplot grid.

### Importance in Machine Learning Pipeline

- **Data Visualization**: Visualizing the distribution of features helps understand the underlying data. This is crucial for identifying patterns, skewness, and potential outliers.
- **Feature Analysis**: Histograms provide insight into the distribution of each feature, indicating whether they are normally distributed, skewed, or have other characteristics. This can guide feature engineering and preprocessing steps, such as normalization or transformation.
- **Understanding Data Quality**: Identifies anomalies or irregularities in the data distribution that might need to be addressed before model training.

### Interpretation of Histograms

- **Normal Distribution**: Features with a bell-shaped curve, indicating a normal distribution. These are generally easier to work with in many machine learning algorithms.
- **Skewed Distribution**: Features that are not symmetrically distributed, which might require transformation (e.g., log transformation) for better model performance.
- **Zero-Centered**: Many features appear to be zero-centered, which is beneficial as it indicates balanced data without extreme values.
- **Bimodal/Multimodal Distributions**: Features with multiple peaks might indicate different regimes or clusters within the data.

### Example Interpretation

From the provided image:

- **Feature 58**: Appears to have a normal distribution.
- **Feature 61**: Shows some skewness with a concentration around certain values.
- **Feature 64**: Has a noticeable spike at zero, indicating many data points are centered around zero.

Overall, this step is essential for exploring and understanding the dataset, which informs subsequent data preprocessing and feature engineering steps in the machine learning pipeline.
"""

# Display the histogram
fig,axes = plt.subplots(nrows=45,ncols=3,figsize=(25,250))

for i in range(2,137):
    sns.distplot(train.iloc[:,i],ax=axes[(i-2)//3,(i-2)%3])

"""- The most signal features seems have a normal gaussian distribution, and they are zero centered.

- The return features, also are zero centred gaussian distributed. Which it can been seen, that the chance to make profit or to lose, is the same at any time of the day .

### Computing and Displaying Feature Correlations

The provided code calculates and displays the correlation between pairs of features in the training dataset for the Jane Street Market Prediction Challenge. Here's a breakdown of the code and its significance:

#### Code Breakdown

1. **Compute Correlation Matrix**:
   ```python
   correlation_table = train[[train.columns[i] for i in range(2, 137)]].corr()
   ```
   - **train.columns[i] for i in range(2, 137)**: Selects columns from `resp_1` to `feature_129` (excluding `date` and `weight`).
   - **train[[...]]**: Creates a DataFrame with the selected columns.
   - **.corr()**: Computes the pairwise Pearson correlation coefficients for the selected features.

2. **Display the Correlation Table**:
   ```python
   correlation_table
   ```
   - Displays the correlation matrix, which shows the correlation coefficients between each pair of features.

#### Interpretation of the Correlation Table

The correlation table provides insight into the relationships between different features. Here’s a brief explanation of some key observations:

- **Strong Positive Correlation**: A value close to 1 indicates a strong positive relationship, meaning as one feature increases, the other also tends to increase.
- **Strong Negative Correlation**: A value close to -1 indicates a strong negative relationship, meaning as one feature increases, the other tends to decrease.
- **Weak or No Correlation**: A value close to 0 indicates little to no linear relationship between the features.

#### Example Interpretation

- **`resp_1` and `resp_2`**: High correlation (0.89), indicating that returns over different time horizons are related.
- **`feature_125` and `feature_121`**: High correlation (0.923), suggesting these features capture similar information.
- **`feature_0` and other features**: Generally low correlations, indicating it might represent a different aspect of the data.

### Importance in Machine Learning Pipeline

- **Feature Selection**: Identifies redundant features that can be removed to reduce model complexity without losing significant information.
- **Multicollinearity**: High correlation between features can cause multicollinearity, leading to instability in model coefficients. Addressing this helps in creating more robust models.
- **Data Understanding**: Provides a deeper understanding of the dataset, guiding feature engineering and preprocessing steps.

### Visualizing Correlation

While the correlation table provides numeric insight, visualizing it can make it easier to interpret. Here’s how you can create a heatmap to visualize the correlations:

```python
plt.figure(figsize=(20, 15))
sns.heatmap(correlation_table, cmap='coolwarm', annot=False)
plt.title('Feature Correlation Heatmap')
plt.show()
```

This heatmap will color-code the correlation coefficients, making it easier to identify highly correlated features at a glance.

#### Example Visual Interpretation

- **Red Areas**: Indicate strong positive correlations.
- **Blue Areas**: Indicate strong negative correlations.
- **White/Light Areas**: Indicate weak or no correlation.

This step is crucial for understanding the relationships between features and informing subsequent steps in the machine learning pipeline, such as feature selection and model building.
"""

# Compute the correlation between pair features.

correlation_table = train[[train.columns[i] for i in range(2,137)]].corr()

# Display the correlation table of pair features.
correlation_table

"""### Explanation of the Function: Detecting Correlated Features

The `detect_correlated_features` function identifies pairs of features in a DataFrame that have a correlation greater than a specified threshold. Here's a breakdown of the function and its significance:

#### Function Definition
```python
def detect_correlated_features(df, threshold=0.5):
    '''
    This function detects features that have a correlation greater than the specified threshold value.

    @param df (DataFrame): The DataFrame containing correlation values between features.
    @param threshold (float): The threshold for detecting correlated features.
    @return correlated (dict): Dictionary summarizing features that have a correlation greater than the specified threshold.
    '''
    correlated = defaultdict(list)
    for col in df.columns:
        dex = list(df.columns).index(col)
        for ind in df.index[dex+1:]:
            if df.loc[col, ind] > threshold:
                correlated[col].append(ind)
                
    return correlated
```

#### Parameters

- **df (DataFrame)**: The DataFrame containing the correlation values between features. Typically, this is the output from the `.corr()` method.
- **threshold (float)**: The correlation threshold above which features are considered highly correlated. Default is 0.5.

#### Return

- **correlated (dict)**: A dictionary where keys are feature names and values are lists of features that have a correlation with the key feature greater than the threshold.

#### Function Workflow

1. **Initialization**:
   ```python
   correlated = defaultdict(list)
   ```
   - Initializes a dictionary where each key will be a feature, and each value will be a list of features correlated with the key feature above the specified threshold.

2. **Iterating Over Columns**:
   ```python
   for col in df.columns:
       dex = list(df.columns).index(col)
   ```
   - Loops through each column in the correlation DataFrame.
   - `dex` captures the index of the current column in the list of columns.

3. **Detecting Correlated Features**:
   ```python
   for ind in df.index[dex+1:]:
       if df.loc[col, ind] > threshold:
           correlated[col].append(ind)
   ```
   - For each column, iterates through the rows starting from the next column to avoid redundant checks.
   - Checks if the correlation value is greater than the threshold.
   - If true, adds the feature to the list of correlated features for the current column.

4. **Returning Correlated Features**:
   ```python
   return correlated
   ```
   - Returns the dictionary of correlated features.

### Importance in Machine Learning Pipeline

- **Feature Selection**: Identifying highly correlated features helps in reducing redundancy. Features with high correlation can be removed to simplify the model and avoid multicollinearity.
- **Model Performance**: Removing correlated features can improve model performance by reducing overfitting and making the model more interpretable.
- **Data Understanding**: Helps understand relationships between features, guiding effective feature engineering and preprocessing.

### Example Usage

```python
correlation_table = train[[train.columns[i] for i in range(2, 137)]].corr()
correlated_features = detect_correlated_features(correlation_table, threshold=0.8)
print(correlated_features)
```

### Example Output

```python
{
    'feature_1': ['feature_5', 'feature_10'],
    'feature_2': ['feature_8'],
    ...
}
```

This output indicates which features are highly correlated with each other, helping in making informed decisions during feature selection and preprocessing in the machine learning pipeline.
"""

def detect_correlated_features(df,threshold=0.5):
    """This function will try detect features who have correlation grower than the introduced
     threshold value.

     @param df(DataFrame): The dataframe who resume the correlation values between features.
     @param threshold(int) : the threshold that the function, will use as reference to detect
                             correlated features.
     @return list(List): list of tuple, who resume features that have correlation grower than
                           the introduced threshold.
     """
    correlated= defaultdict(list)
    for col in df.columns:
        dex = list(df.columns).index(col)
        for ind in df.index[dex+1:] :
            if df.loc[col,ind] > threshold:
               correlated[col].append (ind)

    return correlated

"""### Detecting and Displaying Highly Correlated Features

The following code detects features with a correlation coefficient greater than 0.9 and displays them in a structured format.

#### Code Breakdown

1. **Detect Highly Correlated Features**:
   ```python
   correlated_features = detect_correlated_features(correlation_table, threshold=0.9)
   ```

2. **Prepare Data for Display**:
   ```python
   ax_features = correlated_features.keys()
   ay_features = []
   for f in correlated_features.values():
       ay_features.extend(f)
   ay_features = np.unique(ay_features)
   ```

#### Explanation

1. **Detecting Correlated Features**:
   - The `detect_correlated_features` function is called with a threshold of 0.9 to identify features with a high correlation.

2. **Preparing the Table for Display**:
   - `ax_features`: Extracts the keys from the `correlated_features` dictionary, representing features that have high correlations with other features.
   - `ay_features`: Gathers all the correlated features from the values in the `correlated_features` dictionary and removes duplicates using `np.unique`.

### Displaying the Correlated Features

To display the highly correlated features in a table format, you can use a DataFrame.

```python
# Create a DataFrame to display the correlated features
correlated_pairs = []
for key, values in correlated_features.items():
    for value in values:
        correlated_pairs.append((key, value))

correlated_df = pd.DataFrame(correlated_pairs, columns=['Feature_1', 'Feature_2'])

# Display the DataFrame
correlated_df
```

#### Explanation

- **correlated_pairs**: A list of tuples where each tuple contains a pair of highly correlated features.
- **correlated_df**: A DataFrame created from the list of tuples, with columns `Feature_1` and `Feature_2`.

### Complete Code

```python
# Detect the highly correlated features with coefficient correlation greater than 0.9
correlated_features = detect_correlated_features(correlation_table, threshold=0.9)

# Prepare data for display
ax_features = correlated_features.keys()
ay_features = []
for f in correlated_features.values():
    ay_features.extend(f)
ay_features = np.unique(ay_features)

# Create a DataFrame to display the correlated features
correlated_pairs = []
for key, values in correlated_features.items():
    for value in values:
        correlated_pairs.append((key, value))

correlated_df = pd.DataFrame(correlated_pairs, columns=['Feature_1', 'Feature_2'])

# Display the DataFrame
correlated_df
```

### Output

The `correlated_df` DataFrame will display pairs of features that have a correlation coefficient greater than 0.9, providing a clear view of which features are highly correlated.

#### Example Output

```
   Feature_1  Feature_2
0  feature_125  feature_121
1  feature_125  feature_123
2  feature_125  feature_126
3  feature_125  feature_127
4  feature_125  feature_128
5  feature_125  feature_129
...
```

This table helps identify redundant features, allowing for better feature selection and improving the overall model performance.
"""

# Detect the highly correlated features.Which they had coefficient correlation grower than 0.9.
correlated_features = detect_correlated_features(correlation_table,threshold=0.9)

# Display a table showing the high correlated features.
ax_features = correlated_features.keys()
ay_features = []
for f in correlated_features.values():
    ay_features.extend(f)
ay_features = np.unique(ay_features)

"""### Explanation of the Code: Visualizing Highly Correlated Features

The provided code sets up a heatmap to visualize the highly correlated features identified in the previous steps. Here’s a detailed explanation of each part of the code:

#### Code Breakdown

1. **Set Up the Matplotlib Figure**:
   ```python
   f, ax = plt.subplots(figsize=(70, 50))
   ```
   - **f, ax**: `f` is the figure object, and `ax` is the axes object on which the heatmap will be drawn.
   - **figsize=(70, 50)**: Sets the size of the figure to be very large to accommodate the detailed heatmap.

2. **Create the Heatmap**:
   ```python
   sns.heatmap(correlation_table.loc[ax_features, ay_features], cmap='BrBG', annot=True, square=True, vmin=-1, vmax=1, linewidths=0.5, cbar_kws={"shrink": .5})
   ```
   - **correlation_table.loc[ax_features, ay_features]**: Selects the submatrix of the correlation table that includes only the highly correlated features identified (`ax_features` and `ay_features`).
   - **cmap='BrBG'**: Uses the 'BrBG' colormap, which provides a diverging color scheme useful for showing positive and negative correlations.
   - **annot=True**: Annotates each cell with the correlation coefficient.
   - **square=True**: Ensures each cell is square-shaped.
   - **vmin=-1, vmax=1**: Sets the range of the colormap from -1 to 1, corresponding to the possible values of Pearson correlation coefficients.
   - **linewidths=0.5**: Sets the width of the lines separating each cell in the heatmap.
   - **cbar_kws={"shrink": .5}**: Shrinks the color bar to 50% of its default size for better visual fit.

### Importance of Heatmap Visualization

- **Identifying Strong Relationships**: Heatmaps are effective in visually identifying pairs of features that have strong positive or negative correlations, which can impact feature selection and engineering.
- **Feature Redundancy**: Helps in detecting redundant features that could be removed to simplify the model and avoid multicollinearity.
- **Data Understanding**: Provides a clear and intuitive way to understand complex relationships within the dataset.

### Complete Code for Visualization

Here is the complete code including the previous steps to ensure context:

```python
# Define the function to detect correlated features
def detect_correlated_features(df, threshold=0.9):
    correlated = defaultdict(list)
    for col in df.columns:
        dex = list(df.columns).index(col)
        for ind in df.index[dex+1:]:
            if df.loc[col, ind] > threshold:
                correlated[col].append(ind)
    return correlated

# Compute the correlation matrix
correlation_table = train[[train.columns[i] for i in range(2, 137)]].corr()

# Detect highly correlated features with correlation coefficient greater than 0.9
correlated_features = detect_correlated_features(correlation_table, threshold=0.9)

# Prepare data for display
ax_features = correlated_features.keys()
ay_features = []
for f in correlated_features.values():
    ay_features.extend(f)
ay_features = np.unique(ay_features)

# Set up the matplotlib figure
f, ax = plt.subplots(figsize=(70, 50))

# Create the heatmap
sns.heatmap(correlation_table.loc[ax_features, ay_features], cmap='BrBG', annot=True, square=True, vmin=-1, vmax=1, linewidths=0.5, cbar_kws={"shrink": .5})

# Display the heatmap
plt.show()
```

### Visual Interpretation

- **Dark Green Areas**: Indicate strong positive correlations close to 1.
- **Dark Brown Areas**: Indicate strong negative correlations close to -1.
- **Light Areas**: Indicate weak or no correlation.
- **Annotated Cells**: Provide exact correlation values for better quantitative understanding.

This heatmap helps in identifying and understanding the highly correlated features, which is crucial for feature selection and reducing multicollinearity in the model.
"""

# Set up the matplotlib figure
f , ax = plt.subplots(figsize=(70,50))
sns.heatmap(correlation_table.loc[ax_features,ay_features],cmap='BrBG',annot=True,square=True,vmin=-1,vmax=1,\
            linewidths=0.5,cbar_kws={"shrink": .5})

"""Let's see that we can save only one feature from each pair highly correlated features.

### Explanation of the Code: Dropping Highly Correlated Features

The following code identifies and removes highly correlated features from the training dataset.

#### Step-by-Step Breakdown

1. **Identify Features to Drop**:
   ```python
   features_to_drop = [f for f in ay_features if f not in ax_features]
   ```
   - **ay_features**: List of features that are highly correlated with others.
   - **ax_features**: Keys from the `correlated_features` dictionary, representing features that have high correlations with other features.
   - **features_to_drop**: This list comprehension selects features that are in `ay_features` but not in `ax_features`. These features are deemed redundant and can be dropped.

2. **Create a New DataFrame Excluding Correlated Features**:
   ```python
   train_df = train[[f for f in list(train.columns) if ((f not in features_to_drop) or (f == "resp"))]]
   ```
   - **list(train.columns)**: List of all column names in the training dataset.
   - **f not in features_to_drop**: Keeps features that are not in the `features_to_drop` list.
   - **f == "resp"**: Ensures the `resp` column is retained as it's essential for prediction.
   - **train_df**: New DataFrame that excludes the highly correlated features identified.

### Importance in Machine Learning Pipeline

- **Reducing Redundancy**: Removing highly correlated features simplifies the model, reducing the risk of overfitting and multicollinearity.
- **Improving Efficiency**: With fewer features, the model can train faster and more efficiently.
- **Enhancing Interpretability**: A simpler model with fewer features is easier to understand and interpret.

### Complete Code for Dropping Correlated Features

Here is the complete code including the previous steps to ensure context and continuity:

```python
# Define the function to detect correlated features
def detect_correlated_features(df, threshold=0.9):
    correlated = defaultdict(list)
    for col in df.columns:
        dex = list(df.columns).index(col)
        for ind in df.index[dex+1:]:
            if df.loc[col, ind] > threshold:
                correlated[col].append(ind)
    return correlated

# Compute the correlation matrix
correlation_table = train[[train.columns[i] for i in range(2, 137)]].corr()

# Detect highly correlated features with correlation coefficient greater than 0.9
correlated_features = detect_correlated_features(correlation_table, threshold=0.9)

# Prepare data for display
ax_features = correlated_features.keys()
ay_features = []
for f in correlated_features.values():
    ay_features.extend(f)
ay_features = np.unique(ay_features)

# Set up the matplotlib figure
f, ax = plt.subplots(figsize=(70, 50))

# Create the heatmap
sns.heatmap(correlation_table.loc[ax_features, ay_features], cmap='BrBG', annot=True, square=True, vmin=-1, vmax=1, linewidths=0.5, cbar_kws={"shrink": .5})

# Display the heatmap
plt.show()

# Identify features to drop
features_to_drop = [f for f in ay_features if f not in ax_features]

# Create a new DataFrame excluding correlated features
train_df = train[[f for f in list(train.columns) if ((f not in features_to_drop) or (f == "resp"))]]

# Display the first few rows of the new DataFrame
train_df.head()
```

### Output

This code will provide a new DataFrame `train_df` that has excluded the highly correlated features, retaining essential columns like `resp`. This streamlined dataset is now ready for further preprocessing and model training, ensuring better performance and interpretability.
"""

# List of features to drop because of they are highly correlated
# with others features in the train dataset.
features_to_drop = [f for f in ay_features if f not in ax_features]

# train datas after removing features assigned to drop list of columns.
train_df = train[[ f for f in list(train.columns) if ((f not in features_to_drop) or (f =="resp"))]]

"""Let's now, study the correlation of each feature with the feature named resp, which should be the feature that make deciders to move on for the trading or not.

### Explanation of the Code: Correlation Between 'resp' and Other Features

The provided code calculates and visualizes the correlation between the target variable `resp` and all other features in the training dataset. Here's a detailed breakdown:

#### Code Breakdown

1. **Compute Correlation with 'resp'**:
   ```python
   label_correlation = pd.DataFrame({c: train_df["resp"].corr(train_df[c]) for c in train_df.columns if c != "resp" and c != "feature_0"}, index=["action"])
   ```
   - This line creates a DataFrame `label_correlation` where:
     - Each column is a feature from the dataset (excluding `resp` and `feature_0`).
     - The values are the correlation coefficients between `resp` and the corresponding feature.
   - `index=["action"]` sets the index of the DataFrame to "action".

2. **Visualization Setup**:
   ```python
   l = len(list(label_correlation.columns))  # Compute the number of features in label_correlation
   col = list(label_correlation.columns)  # List of column names of label_correlation
   ```
   - `l` stores the number of features in `label_correlation`.
   - `col` contains the names of these features.

3. **Divide Features for Clear Visualization**:
   ```python
   fig, axes = plt.subplots(nrows=5, ncols=1, figsize=(60, 30))
   level = [0, int(l / 5), int(2 * l / 5), int(3 * l / 5), int(4 * l / 5), l]
   ```
   - This sets up a figure with 5 subplots arranged vertically (`nrows=5, ncols=1`).
   - `figsize=(60, 30)` sets the overall figure size.
   - `level` divides the list of features into 5 equal parts for clearer visualization.

4. **Create Heatmaps**:
   ```python
   for i in range(len(level) - 1):
       sns.heatmap(label_correlation.loc[:, col[level[i]:level[i + 1]]], annot=True, cmap='BrBG', linewidths=0.5, vmin=-1, vmax=1, cbar_kws={"shrink": .5}, square=True, ax=axes[i])
   ```
   - This loop creates a heatmap for each subset of features.
   - `sns.heatmap(...)` generates the heatmap.
   - `label_correlation.loc[:, col[level[i]:level[i + 1]]]` selects the subset of features for the current heatmap.
   - `annot=True` annotates each cell with the correlation coefficient.
   - `cmap='BrBG'` sets the colormap.
   - `linewidths=0.5` sets the width of the lines separating each cell.
   - `vmin=-1, vmax=1` sets the range of the colormap.
   - `cbar_kws={"shrink": .5}` shrinks the color bar for better fit.
   - `ax=axes[i]` specifies the subplot to draw the heatmap on.

### Importance in Machine Learning Pipeline

- **Feature Selection**: Understanding the correlation between `resp` and other features helps identify the most relevant features for prediction.
- **Model Insight**: Visualizing correlations can provide insights into which features have the strongest relationship with the target variable, potentially guiding feature engineering and model tuning.
- **Data Understanding**: Helps in comprehending the data structure and relationships, which is crucial for building robust predictive models.

### Complete Code for Visualization

Here is the complete code, including context from the previous steps:

```python
# Define the function to detect correlated features
def detect_correlated_features(df, threshold=0.9):
    correlated = defaultdict(list)
    for col in df.columns:
        dex = list(df.columns).index(col)
        for ind in df.index[dex+1:]:
            if df.loc[col, ind] > threshold:
                correlated[col].append(ind)
    return correlated

# Compute the correlation matrix
correlation_table = train[[train.columns[i] for i in range(2, 137)]].corr()

# Detect highly correlated features with correlation coefficient greater than 0.9
correlated_features = detect_correlated_features(correlation_table, threshold=0.9)

# Prepare data for display
ax_features = correlated_features.keys()
ay_features = []
for f in correlated_features.values():
    ay_features.extend(f)
ay_features = np.unique(ay_features)

# Identify features to drop
features_to_drop = [f for f in ay_features if f not in ax_features]

# Create a new DataFrame excluding correlated features
train_df = train[[f for f in list(train.columns) if ((f not in features_to_drop) or (f == "resp"))]]

# Compute the correlation between 'resp' and the rest of the features
label_correlation = pd.DataFrame({c: train_df["resp"].corr(train_df[c]) for c in train_df.columns if c != "resp" and c != "feature_0"}, index=["action"])

# Visualize the correlation in a table named label_correlation
l = len(list(label_correlation.columns))  # Compute the number of features in label_correlation
col = list(label_correlation.columns)  # List of column names of label_correlation

# Divide the features into 5 parts for clearer charts
fig, axes = plt.subplots(nrows=5, ncols=1, figsize=(60, 30))
level = [0, int(l / 5), int(2 * l / 5), int(3 * l / 5), int(4 * l / 5), l]

for i in range(len(level) - 1):
    sns.heatmap(label_correlation.loc[:, col[level[i]:level[i + 1]]], annot=True, cmap='BrBG', linewidths=0.5, vmin=-1, vmax=1, cbar_kws={"shrink": .5}, square=True, ax=axes[i])

plt.show()
```

### Visual Interpretation

- **Heatmaps**: Show the correlation between `resp` and other features, divided into five parts for clarity.
- **Color Coding**:
  - **Dark Green**: Strong positive correlation.
  - **Dark Brown**: Strong negative correlation.
  - **Light Areas**: Weak or no correlation.
- **Annotated Cells**: Provide exact correlation values for better quantitative understanding.

This visualization helps in identifying and understanding the most significant features related to the target variable, aiding in effective feature selection and model development.
"""

# Compute the correlation betwen the features named resp, and the reste of features
label_correlation = pd.DataFrame({c:train_df["resp"].corr(train_df[c]) for\
                                  c in train_df.columns if c!="resp" and c!="feature_0"},index=["action"])

# Visualize the correlation in a table named label_correlation.
l = len(list(label_correlation.columns)) # compute the number  of features for label_correlation dataset
col = list(label_correlation.columns) # list of columns names of label_correlation dataset

# Because of , the high number of features in our dataset, we will try divide them into 5,
# in order to get more clear chart.
fig ,axes = plt.subplots(nrows=5,ncols=1,figsize=(60,30))
level = [0,int(l/5),int(2*l/5),int(3*l/5),int(4*l/5),l]
for i in range(len(level)-1):
    sns.heatmap(label_correlation.loc[:,col[level[i]:level[i+1]]],annot=True,cmap='BrBG',\
                linewidths=0.5,vmin=-1,vmax=1,cbar_kws={"shrink": .5},square=True,ax=axes[i])

"""Except resp_{1,2,3,4} values that represent returns over different time horizons, there is no evident linear relation between retained features and the feature of return.

### Explanation of the Code: Analyzing Correlation Between `feature_0` and Target Values

The provided code examines the correlation between a binary feature named `feature_0` and the target values derived from `resp` and its related columns. Here's a step-by-step breakdown:

#### Step-by-Step Breakdown

1. **Creating the Target Variable**:
   ```python
   train_df.loc[:, "target"] = list(((train_df["resp"] > 0) & (train_df["resp_1"] > 0) & (train_df["resp_2"] > 0) & (train_df["resp_3"] > 0) & (train_df["resp_4"] > 0)).astype("int"))
   train_df.loc[:, "vl"] = list(train_df["target"].values)
   ```
   - A new column `target` is created, which is `1` if `resp` and all related columns (`resp_1`, `resp_2`, `resp_3`, `resp_4`) are greater than 0, and `0` otherwise.

2. **Creating a Pivot Table**:
   ```python
   pvt_table = train_df[["feature_0", "target"]].pivot_table(index=["feature_0"], columns=["target"], aggfunc=len)
   ```
   - This pivot table shows the count of instances for each combination of `feature_0` and `target`.

3. **Normalization and Calculations**:
   ```python
   tx = train_df["feature_0"].value_counts()
   ty = train_df["target"].value_counts()
   tx = pd.DataFrame(tx)
   ty = pd.DataFrame(ty)
   n = len(train_df)
   tx.columns = ["values"]
   ty.columns = ["values"]

   cnt = tx.dot(ty.T) / n
   ind = cnt.index
   pvt_table = pvt_table.loc[ind, :]
   mesure = (cnt - pvt_table)**2 / cnt
   xin = mesure.sum().sum()
   ```

   - `tx` and `ty` calculate the value counts of `feature_0` and `target`, respectively.
   - `cnt` computes the expected count of occurrences for each combination if `feature_0` and `target` were independent.
   - `mesure` calculates the Chi-square statistic for each cell, measuring the deviation from independence.
   - `xin` is the sum of these Chi-square values, representing the total correlation measure between `feature_0` and `target`.

4. **Cleaning Up**:
   ```python
   del(train_df["target"])
   del(train_df["vl"])
   ```

   - Deletes the temporary columns from the DataFrame to clean up.

5. **Visualizing the Correlation**:
   ```python
   fig = plt.figure(figsize=(12, 8))
   sns.heatmap(mesure, annot=True, linewidths=0.5, cmap="BrBG", vmin=0, vmax=1)
   plt.title("Correlation table between feature_0 and the target value", size=15, color="red")
   print("The total correlation between feature_0 and the target equal to {}".format(xin))
   ```

   - Creates a heatmap to visualize the Chi-square statistic for each cell.
   - Titles the heatmap and prints the total correlation measure.

### Importance in Machine Learning Pipeline

- **Feature Evaluation**: Understanding the correlation between a feature and the target variable is crucial for evaluating its predictive power.
- **Feature Selection**: Features with low correlation to the target may be dropped to simplify the model and improve performance.
- **Data Understanding**: Provides insight into how individual features contribute to the target variable, guiding feature engineering and model tuning.

### Complete Code for Correlation Analysis

Here is the complete code, including context and necessary comments:

```python
# Create the target variable
train_df.loc[:, "target"] = list(((train_df["resp"] > 0) & (train_df["resp_1"] > 0) & (train_df["resp_2"] > 0) & (train_df["resp_3"] > 0) & (train_df["resp_4"] > 0)).astype("int"))
train_df.loc[:, "vl"] = list(train_df["target"].values)

# Create a pivot table
pvt_table = train_df[["feature_0", "target"]].pivot_table(index=["feature_0"], columns=["target"], aggfunc=len)

# Normalize and calculate expected counts
tx = train_df["feature_0"].value_counts()
ty = train_df["target"].value_counts()
tx = pd.DataFrame(tx)
ty = pd.DataFrame(ty)
n = len(train_df)
tx.columns = ["values"]
ty.columns = ["values"]

cnt = tx.dot(ty.T) / n
ind = cnt.index
pvt_table = pvt_table.loc[ind, :]
mesure = (cnt - pvt_table)**2 / cnt
xin = mesure.sum().sum()

# Clean up temporary columns
del(train_df["target"])
del(train_df["vl"])

# Visualize the correlation
fig = plt.figure(figsize=(12, 8))
sns.heatmap(mesure, annot=True, linewidths=0.5, cmap="BrBG", vmin=0, vmax=1)
plt.title("Correlation table between feature_0 and the target value", size=15, color="red")
print("The total correlation between feature_0 and the target equal to {}".format(xin))
```

### Visual Interpretation

- **Heatmap**: Shows the Chi-square statistic for the combinations of `feature_0` and `target`.
  - **Dark Brown**: Indicates higher Chi-square values, suggesting a stronger deviation from independence.
  - **Lighter Colors**: Suggest lower Chi-square values, indicating weaker correlation.
- **Total Correlation Measure**: `xin` provides an aggregate measure of how strongly `feature_0` is correlated with the target variable, offering a quantitative assessment of its relevance.

This analysis helps in assessing the importance of `feature_0` in predicting the target variable, guiding feature selection and model refinement.
"""

# correlation between the binary feature named feature_0 and the target values.
train_df.loc[:,"target"] = list(((train_df["resp"] > 0) & (train_df["resp_1"] >0) & (train_df["resp_2"]>0)\
                     & (train_df["resp_3"]>0) & (train_df["resp_4"]>0)).astype("int"))
train_df.loc[:,"vl"] = list(train_df["target"].values)
pvt_table=train_df[["feature_0","target"]].pivot_table(index=["feature_0"],columns=["target"],aggfunc=len)
#nb_1 = len(train_df.loc[train_df["target"]==1,:])
#nb_0 = len(train_df.loc[train_df["target"]==0,:])
#pvt_table[1] = pvt_table[1]/nb_1
#pvt_table[0] = pvt_table[0]/nb_0
tx= train_df["feature_0"].value_counts()
ty = train_df["target"].value_counts()
tx = pd.DataFrame(tx)
ty = pd.DataFrame(ty)
n = len(train_df)
tx.columns = ["values"]
ty.columns = ["values"]

cnt = tx.dot(ty.T)/n
ind = cnt.index
pvt_table = pvt_table.loc[ind,:]
mesure = (cnt - pvt_table)**2/cnt
xin = mesure.sum().sum()

del(train_df["target"])
del(train_df["vl"])
#del(nb_1)
#del(nb_0)
fig = plt.figure(figsize=(12,8))
sns.heatmap(mesure,annot=True,linewidths=0.5,cmap="BrBG",vmin=0,vmax=1)
plt.title("Correlation table between feature_0 and the target value ",size=15,color="red")
print("The total correlation between feature_0 and the target equal to {}".format(xin))
#del(pvt_table)

"""<font color=redblue> We can conclude that, there is no correlation between feature_0 and the target values.

<h2 id=missing_values> Missing Values:

### Display Missing Values Before Imputation

The following code calculates and displays the number of missing values in each column of the `train_df` DataFrame before performing any imputation.

#### Step-by-Step Breakdown

1. **Calculate Missing Values**:
   ```python
   missing_b_imputation = pd.DataFrame({c: (train_df[c].isna().sum()) for c in train_df.columns}, index=["% missing values"])
   ```
   - Creates a DataFrame `missing_b_imputation` where each column represents a feature in `train_df`.
   - For each feature, the value is the number of missing entries (NaN values).
   - The index of the DataFrame is set to `"% missing values"` for clarity.

2. **Calculate Total Missing Values**:
   ```python
   total_missing_values = missing_b_imputation.sum().sum()
   ```
   - Sums the missing values for each column and then sums those totals to get the total number of missing values in the entire DataFrame.

3. **Print Total Missing Values**:
   ```python
   print("The number of missing values in the train_df dataframe before imputation processing: {}".format(total_missing_values))
   ```

4. **Display the Missing Values DataFrame**:
   ```python
   missing_b_imputation
   ```

### Complete Code for Displaying Missing Values

```python
# Calculate and display the number of missing values before imputation
missing_b_imputation = pd.DataFrame({c: (train_df[c].isna().sum()) for c in train_df.columns}, index=["% missing values"])

# Calculate total missing values
total_missing_values = missing_b_imputation.sum().sum()

# Print total missing values
print("The number of missing values in the train_df dataframe before imputation processing: {}".format(total_missing_values))

# Display the DataFrame of missing values
missing_b_imputation
```

### Output Interpretation

The output provides a detailed view of missing values in the `train_df` DataFrame:

- **Total Missing Values**: 4,926,085
- **Missing Values Per Column**: Shows the number of missing values for each feature.

#### Example Output

```plaintext
The number of missing values in the train_df dataframe before imputation processing: 4926085

                date  weight  resp_1  resp_2  resp_3  resp_4  resp  feature_0  feature_1  feature_2  ...  feature_118  feature_120  feature_121  feature_122  feature_123  feature_124  feature_125  feature_126  feature_127  ts_id
% missing values     0       0       0       0       0       0     0          0         0          0  ...         6683         69854        69854        223          223          16083        16083        8853         8853         0
```

### Importance of Handling Missing Values

- **Data Quality**: Missing values can significantly affect the performance of machine learning models. Identifying and handling them is crucial.
- **Imputation Strategies**: Depending on the percentage of missing values, different imputation techniques can be used, such as mean/median imputation, forward/backward filling, or model-based imputation.
- **Model Robustness**: Properly handling missing values ensures that the model is trained on complete and consistent data, leading to more reliable and robust predictions.

### Next Steps: Imputation

After identifying the missing values, the next logical step is to handle them through imputation. Here are a few common strategies:

1. **Mean/Median Imputation**:
   ```python
   train_df.fillna(train_df.mean(), inplace=True)
   ```

2. **Forward/Backward Fill**:
   ```python
   train_df.fillna(method='ffill', inplace=True)  # Forward fill
   train_df.fillna(method='bfill', inplace=True)  # Backward fill
   ```

3. **Model-Based Imputation** (using methods like KNN, regression, or more sophisticated techniques).

Handling missing values appropriately will improve the quality of the dataset, which in turn can enhance the performance of machine learning models built using this data.
"""

# Display train_df missing values before imputations.
missing_b_imputation = pd.DataFrame({c:(train_df[c].isna().sum()) for c in train_df.columns},index=["% missing values"])
print("The number of missing values in the train_df dataframe before imputation processing :{}".format(\
                                                                                                      missing_b_imputation.sum().sum()))
missing_b_imputation

"""### Explanation of the Code: Handling Missing Values with Linear Regression Imputation

The provided code identifies columns with missing values in the `train_df` DataFrame and uses a linear regression model to impute those missing values using correlated features. Here's a detailed breakdown:

#### Step-by-Step Breakdown

1. **Identify Features with Missing Values**:
   ```python
   features_with_missing_values = []  # List to store features with missing values

   for f in list(train_df.columns):
       if missing_b_imputation.loc["% missing values", f] > 0:
           features_with_missing_values.append(f)
   ```
   - Loops through all columns in `train_df`.
   - If a column has missing values (as identified in `missing_b_imputation`), it is added to the `features_with_missing_values` list.

2. **Train Linear Regression Model for Imputation**:
   ```python
   for f in features_with_missing_values:
       model = LinearRegression()
       if len(correlated_features[f]) > 0:
           correlated = correlated_features[f][0]
           if correlated in train.columns:
               model.fit(train.loc[(train[correlated].notna()) & (train[f].notna()), correlated].values.reshape(-1, 1),
                         train.loc[(train[correlated].notna()) & (train[f].notna()), f])
               values_to_impute = train_df.loc[(train[f].isna()) & (train[correlated].notna()), f]
               imputer = train.loc[(train[f].isna()) & (train[correlated].notna()), correlated].values
               if (len(values_to_impute) > 0) & (len(imputer) > 0):
                   train_df.loc[(train[f].isna()) & (train[correlated].notna()), f] = model.predict(
                       train.loc[(train[f].isna()) & (train[correlated].notna()), correlated].values.reshape(-1, 1))
   ```

   - **Model Initialization**:
     - For each feature with missing values, a `LinearRegression` model is initialized.
   
   - **Check Correlated Features**:
     - If there are correlated features available for the feature with missing values, the first correlated feature is selected.
   
   - **Fit Model**:
     - The model is trained using non-missing values of the feature and its correlated feature.
   
   - **Predict Missing Values**:
     - The model predicts missing values for the feature using available values of its correlated feature.
     - The predicted values are used to fill the missing values in `train_df`.

### Importance in Machine Learning Pipeline

- **Improved Data Quality**: Imputing missing values enhances the quality of the dataset, ensuring that all features have complete data for model training.
- **Leveraging Correlation**: Using correlated features for imputation ensures that the imputed values are consistent with the relationships observed in the data.
- **Model Robustness**: Properly handling missing values contributes to more robust and reliable predictive models.

### Complete Code for Linear Regression Imputation

```python
# Identify features with missing values in the new dataset named train_df
features_with_missing_values = []  # List of features with missing values

for f in list(train_df.columns):
    if missing_b_imputation.loc["% missing values", f] > 0:
        features_with_missing_values.append(f)

# Train a linear regression model for each feature with missing values using its correlated feature
for f in features_with_missing_values:
    model = LinearRegression()
    if len(correlated_features[f]) > 0:
        correlated = correlated_features[f][0]
        if correlated in train.columns:
            # Train the model using non-missing values
            model.fit(train.loc[(train[correlated].notna()) & (train[f].notna()), correlated].values.reshape(-1, 1),
                      train.loc[(train[correlated].notna()) & (train[f].notna()), f])
            # Predict and impute missing values
            values_to_impute = train_df.loc[(train[f].isna()) & (train[correlated].notna()), f]
            imputer = train.loc[(train[f].isna()) & (train[correlated].notna()), correlated].values
            if (len(values_to_impute) > 0) & (len(imputer) > 0):
                train_df.loc[(train[f].isna()) & (train[correlated].notna()), f] = model.predict(
                    train.loc[(train[f].isna()) & (train[correlated].notna()), correlated].values.reshape(-1, 1))
```

### Explanation of Key Steps

1. **Model Training**: The linear regression model is trained using non-missing values of the feature and its correlated feature. This step ensures that the imputed values maintain the statistical relationship observed in the data.
2. **Prediction and Imputation**: The model predicts the missing values using the correlated feature, and these predicted values are used to fill in the missing entries in the `train_df`.

### Conclusion

This approach leverages the correlation between features to impute missing values, ensuring the imputed data is consistent with the observed patterns in the dataset. This improves data quality and contributes to building more reliable and accurate machine learning models.
"""

# identify , which column has missing values in the new dataset named train_df

features_with_missing_values = [] # list of features , has missing values.

for f in list(train_df.columns):
    if missing_table.loc["% missing values",f] > 0 :
        features_with_missing_values.append(f)

# train a linear model regression for each feature, had missing values with his one of correlated
# feature
for f in features_with_missing_values :
    model = LinearRegression()
    if  len(correlated_features[f]) > 0 :
        correlated = correlated_features[f][0]
        if correlated in train.columns :
           model.fit(train.loc[(train[correlated].notna()) & (train[f].notna()),correlated].values.reshape(-1,1),\
              train.loc[(train[correlated].notna()) & (train[f].notna()),f])
           values_to_impute = train_df.loc[(train[f].isna()) & (train[correlated].notna()),f]
           imputer = train.loc[(train[f].isna())&(train[correlated].notna()),correlated].values
           if (len(values_to_impute) > 0) & (len(imputer) > 0) :
              train_df.loc[(train[f].isna()) & (train[correlated].notna()),f] = model.predict(train.loc[(train[f].isna())&(train[correlated].notna()),correlated].values.\
                                                      reshape(-1,1))

"""### Display Missing Values After Imputation

The following code calculates and displays the number of missing values in each column of the `train_df` DataFrame after performing linear regression imputation.

#### Step-by-Step Breakdown

1. **Calculate Missing Values**:
   ```python
   missing_a_imputation = pd.DataFrame({c: (train_df[c].isna().sum()) for c in train_df.columns}, index=["Number missing values"])
   ```
   - Creates a DataFrame `missing_a_imputation` where each column represents a feature in `train_df`.
   - For each feature, the value is the number of missing entries (NaN values).
   - The index of the DataFrame is set to `"Number missing values"` for clarity.

2. **Calculate Total Missing Values**:
   ```python
   total_missing_values_after = missing_a_imputation.sum().sum()
   ```

3. **Print Total Missing Values**:
   ```python
   print("The number of missing values in the train_df dataframe after imputation processing: {}".format(total_missing_values_after))
   ```

4. **Display the Missing Values DataFrame**:
   ```python
   missing_a_imputation
   ```

### Complete Code for Displaying Missing Values After Imputation

```python
# Calculate and display the number of missing values after imputation
missing_a_imputation = pd.DataFrame({c: (train_df[c].isna().sum()) for c in train_df.columns}, index=["Number missing values"])

# Calculate total missing values after imputation
total_missing_values_after = missing_a_imputation.sum().sum()

# Print total missing values after imputation
print("The number of missing values in the train_df dataframe after imputation processing: {}".format(total_missing_values_after))

# Display the DataFrame of missing values
missing_a_imputation
```

### Output Interpretation

The output provides a detailed view of missing values in the `train_df` DataFrame after imputation:

- **Total Missing Values**: 4,624,136
- **Missing Values Per Column**: Shows the number of missing values for each feature after imputation.

#### Example Output

```plaintext
The number of missing values in the train_df dataframe after imputation processing: 4624136

                    date  weight  resp_1  resp_2  resp_3  resp_4  resp  feature_0  feature_1  feature_2  ...  feature_118  feature_120  feature_121  feature_122  feature_123  feature_124  feature_125  feature_126  feature_127  ts_id
Number missing values     0       0       0       0       0       0     0          0         0          0  ...         6683         455          455          141          212           8775         8775         1882          1882       0
```

### Significance

- **Reduction in Missing Values**: The number of missing values has decreased significantly after the imputation process, indicating that the linear regression imputation successfully filled in many of the missing entries.
- **Remaining Missing Values**: Some columns still have missing values. These can be handled with additional imputation methods or by dropping columns/rows depending on their impact on the model.

### Next Steps: Further Imputation or Analysis

1. **Further Imputation**:
   - For features still with missing values, consider using other imputation techniques such as:
     - **Mean/Median Imputation**:
       ```python
       train_df.fillna(train_df.mean(), inplace=True)
       ```
     - **Forward/Backward Fill**:
       ```python
       train_df.fillna(method='ffill', inplace=True)  # Forward fill
       train_df.fillna(method='bfill', inplace=True)  # Backward fill
       ```

2. **Analysis**:
   - Assess the impact of imputed values on model performance.
   - Validate the consistency and correctness of imputed values.

By continually refining the dataset and addressing missing values, you can ensure that the resulting machine learning models are robust and accurate.
"""

#Display train_df missing values after imputation with linear regression
missing_a_imputation = pd.DataFrame({c:(train_df[c].isna().sum()) for c in train_df.columns},index=["Number missing values"])
print("The number of missing values in the train_df dataframe after imputation processing :{}".format(\
                                                                                                      missing_a_imputation.sum().sum()))
missing_a_imputation

"""==> Although this approach to remplace missing values , is very efficient .It was able to impute only about 6% of the prior number of missing values."""

# for the rest of the missing values, we will use the mean value as imputer for each features.
for f in features_with_missing_values:
    train_df.fillna(train_df[f].mean(),inplace=True)

"""<h2 id=feature_engineering> Feature Engineering </h2>

The feature that we want to predict through this project , is the feature which can give signal to the investor to move on with the trading or not. The decision of the investor is based on the return of the trading ,so they should predict which one can make profit. Therefore we will create new feature named "action" wich give 1 when there is positive return and 0 if it is not.
"""

#resp_cols = ['resp_1', 'resp_2', 'resp_3', 'resp', 'resp_4']

"""In order to enhance the predictive quality of our model, we will check the correlation between the global return and the differents returns in differents time horizons.

### Feature Engineering: Creating the `action` Feature

In this project, the goal is to create a feature that signals to an investor whether to proceed with a trade based on positive returns. The new feature, `action`, is set to `1` if there is a positive return (`resp > 0`) and `0` otherwise.

#### Step-by-Step Breakdown

1. **Creating the `action` Feature**:
   ```python
   returns = ["resp_1", "resp_2", "resp_3", "resp_4", "resp"]
   datas = pd.DataFrame({c: (train_df[c] > 0).astype("int") for c in returns})
   datas["val"] = datas.loc[:, "resp"].values
   ```

   - `returns`: List of return features.
   - `datas`: DataFrame containing binary values indicating positive returns (`1` if return > 0, otherwise `0`).

2. **Visualizing Correlations**:
   ```python
   fig, ax = plt.subplots(2, 2, figsize=(15, 12))
   for i in range(len(returns) - 1):
       k = i // 2
       l = i % 2
       piv_resp_1_resp = pd.pivot_table(datas, index=returns[i], columns="resp", values="val", aggfunc="count")
       ty = datas["resp"].value_counts()
       tx = datas[returns[i]].value_counts()
       tx = pd.DataFrame(tx)
       ty = pd.DataFrame(ty)
       ind = piv_resp_1_resp.index
       col = piv_resp_1_resp.columns
       tx.columns = ["values"]
       ty.columns = ["values"]
       n = len(datas)
       cnt = tx.dot(ty.T) / n
       cnt = cnt.loc[ind, col]
       mesure = (cnt - piv_resp_1_resp) ** 2 / cnt
       xid = mesure.sum().sum()
       sns.heatmap(mesure, annot=True, linewidths=0.5, cmap="BrBG", ax=ax[k, l], vmin=0, vmax=1)
       ax[k, l].set_title("The Correlation Table Between the feature resp and {} ".format(returns[i]), size=12, color="red")
       print("The total correlation between the feature {} and the resp feature equal to {}".format(returns[i], xid))
   ```

   - **Subplots**: Creates a 2x2 grid of subplots for visualizing the correlation heatmaps.
   - **Pivot Table**: Creates pivot tables to compute counts for positive and negative returns for each return feature.
   - **Heatmap**: Visualizes the correlation between `resp` and each return feature using heatmaps.

### Importance in Machine Learning Pipeline

- **Feature Creation**: The `action` feature is crucial for predicting whether an investor should make a trade based on expected returns.
- **Correlation Analysis**: Understanding the correlation between `resp` and other return features helps in evaluating their importance and impact on the `action` feature.
- **Visualization**: Heatmaps provide a clear visual representation of correlations, aiding in data understanding and feature selection.

### Complete Code for Feature Engineering and Visualization

```python
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Create the 'action' feature
returns = ["resp_1", "resp_2", "resp_3", "resp_4", "resp"]
datas = pd.DataFrame({c: (train_df[c] > 0).astype("int") for c in returns})
datas["val"] = datas.loc[:, "resp"].values

# Visualize correlations
fig, ax = plt.subplots(2, 2, figsize=(15, 12))
for i in range(len(returns) - 1):
    k = i // 2
    l = i % 2
    piv_resp_1_resp = pd.pivot_table(datas, index=returns[i], columns="resp", values="val", aggfunc="count")
    ty = datas["resp"].value_counts()
    tx = datas[returns[i]].value_counts()
    tx = pd.DataFrame(tx)
    ty = pd.DataFrame(ty)
    ind = piv_resp_1_resp.index
    col = piv_resp_1_resp.columns
    tx.columns = ["values"]
    ty.columns = ["values"]
    n = len(datas)
    cnt = tx.dot(ty.T) / n
    cnt = cnt.loc[ind, col]
    mesure = (cnt - piv_resp_1_resp) ** 2 / cnt
    xid = mesure.sum().sum()
    sns.heatmap(mesure, annot=True, linewidths=0.5, cmap="BrBG", ax=ax[k, l], vmin=0, vmax=1)
    ax[k, l].set_title("The Correlation Table Between the feature resp and {} ".format(returns[i]), size=12, color="red")
    print("The total correlation between the feature {} and the resp feature equal to {}".format(returns[i], xid))
plt.tight_layout()
plt.show()
```

### Visual Interpretation

- **Heatmaps**: Show the correlation between `resp` and other return features.
  - **Dark Colors**: Indicate higher correlation.
  - **Light Colors**: Indicate lower correlation.
- **Correlation Values**: Annotated on the heatmap, providing exact correlation measures.

### Summary

By creating the `action` feature and analyzing its correlation with other return features, you can enhance the predictive power of the model and ensure that it provides reliable signals for investors. This approach is integral to feature engineering and model development in quantitative finance.

### Explanation of the Image and Its Meaning

The image shows four heatmaps representing the correlation between the primary target feature `resp` and its related features `resp_1`, `resp_2`, `resp_3`, and `resp_4`. Each subplot is a 2x2 grid indicating the relationship between binary versions of these features.

#### Key Points

1. **Binary Representation**:
   - Each feature (`resp`, `resp_1`, `resp_2`, `resp_3`, `resp_4`) is converted to a binary form:
     - `1` if the value is positive (indicating a positive return).
     - `0` if the value is non-positive (indicating a non-positive return).

2. **Heatmap Structure**:
   - **Rows**: Represent the binary values of the related feature (`resp_1`, `resp_2`, `resp_3`, `resp_4`).
   - **Columns**: Represent the binary values of the primary feature (`resp`).
   - **Cells**: Contain counts indicating how many times each combination of binary values occurs.

3. **Color Coding**:
   - The color gradient ranges from dark to light, indicating different levels of count values.
   - Dark colors represent higher counts, showing stronger correlations.

4. **Total Correlation Values**:
   - Calculated using a Chi-square statistic, which measures how the observed counts deviate from expected counts under the assumption of independence.
   - Higher values indicate a stronger relationship between the two features.

#### Subplots Breakdown

1. **Top-Left (resp_1 and resp)**:
   - Shows the correlation between `resp_1` and `resp`.
   - Total correlation value: 203,362.03.
   - Indicates a significant relationship, as seen by the color intensity.

2. **Top-Right (resp_2 and resp)**:
   - Shows the correlation between `resp_2` and `resp`.
   - Total correlation value: 367,380.53.
   - Indicates a stronger relationship compared to `resp_1`, as evidenced by a higher total correlation value.

3. **Bottom-Left (resp_3 and resp)**:
   - Shows the correlation between `resp_3` and `resp`.
   - Total correlation value: 935,181.26.
   - Indicates an even stronger relationship, with more intense colors.

4. **Bottom-Right (resp_4 and resp)**:
   - Shows the correlation between `resp_4` and `resp`.
   - Total correlation value: 1,582,107.56.
   - Indicates the strongest relationship among the four, with the highest total correlation value.

### Meaning and Implications

- **Correlation Insight**: The heatmaps and their total correlation values provide a visual and quantitative understanding of how closely related each `resp_x` feature is to the primary `resp` feature.
- **Feature Importance**: Higher correlation values suggest that the related feature (`resp_x`) could be an important predictor of the primary feature (`resp`). This insight can guide feature selection and model building.
- **Decision-Making**: For investors, understanding these correlations helps in making informed decisions about trades. A strong correlation between the returns (`resp_x`) and the overall return (`resp`) indicates that observing positive returns in `resp_x` is a strong signal for a positive `resp`.

These visualizations are crucial in understanding the dataset's structure and relationships, informing better feature engineering and model development practices.
"""

#
returns = ["resp_1","resp_2","resp_3","resp_4","resp"]

datas = pd.DataFrame({c:(train_df[c]>0).astype("int") for c in returns})
datas["val"] = datas.loc[:,"resp"].values
fig , ax = plt.subplots(2,2,figsize= (15,12))
for i in range(len(returns)-1):
    k = i // 2
    l = i % 2
    piv_resp_1_resp = pd.pivot_table(datas,index= returns[i],columns="resp",values="val",aggfunc="count")
    ty = datas["resp"].value_counts()
    tx = datas[returns[i]].value_counts()
    tx = pd.DataFrame(tx)
    ty = pd.DataFrame(ty)
    ind = piv_resp_1_resp.index
    col = piv_resp_1_resp.columns
    tx.columns = ["values"]
    ty.columns = ["values"]
    n = len(datas)
    cnt = tx.dot(ty.T)/n
    cnt = cnt.loc[ind,col]
    mesure = (cnt - piv_resp_1_resp) ** 2 /cnt
    xid = mesure.sum().sum()
    mesure = mesure
    sns.heatmap(mesure,annot=True,linewidths=0.5,cmap="BrBG",ax=ax[k,l],vmin=0,vmax=1)
    ax[k,l].set_title("The Correlation Table Between the feature resp and {} ".\
                      format(returns[i]),size=12,color="red")
    print("The total correlation between the feature {} and the resp feature equal to {}".\
          format(returns[i],xid))

"""<font color=redblue> We can conclude that there is high correlation between the return in the different horizons time and the return feature, which confirm that there features are highly correlated with the return feature. For this reason we will choice our feature action , that will be our target, as the combinaison of all theses features.

### Feature Engineering: Defining the `action` Feature

In this step, we create a new feature called `action`, which signals to investors whether to proceed with a trade based on the returns. The `action` feature is set to `1` if all return columns (`resp`, `resp_1`, `resp_2`, `resp_3`, `resp_4`) are positive, indicating a profitable trade, and `0` otherwise.

#### Code to Define the `action` Feature

```python
# Define the new feature 'action'
train_df["action"] = ((train_df["resp"] > 0) &
                      (train_df["resp_1"] > 0) &
                      (train_df["resp_2"] > 0) &
                      (train_df["resp_3"] > 0) &
                      (train_df["resp_4"] > 0)).astype("int")
```

#### Explanation of the Code

1. **Creating the `action` Feature**:
   - `train_df["action"]`: Creates a new column `action` in the `train_df` DataFrame.
   - `((train_df["resp"] > 0) & (train_df["resp_1"] > 0) & (train_df["resp_2"] > 0) & (train_df["resp_3"] > 0) & (train_df["resp_4"] > 0))`: Checks if all the return columns (`resp`, `resp_1`, `resp_2`, `resp_3`, `resp_4`) are positive.
   - `.astype("int")`: Converts the boolean result to an integer (`1` for `True`, `0` for `False`).

### Importance in the Machine Learning Pipeline

- **Target Variable**: The `action` feature serves as a target variable for training models that predict whether an investor should make a trade.
- **Feature Engineering**: Creating such derived features can significantly enhance the predictive power of machine learning models.
- **Decision Making**: Provides a clear and actionable signal for investors based on the aggregated returns.

### Example Use Case

With the `action` feature defined, you can now proceed to train a machine learning model to predict `action` based on other features in the dataset. This prediction will help investors make informed decisions about whether to execute trades.

### Complete Code Context

Here's the complete context of the code including the new `action` feature definition:

```python
import pandas as pd

# Assuming train_df is already defined and loaded with data

# Define the new feature 'action'
train_df["action"] = ((train_df["resp"] > 0) &
                      (train_df["resp_1"] > 0) &
                      (train_df["resp_2"] > 0) &
                      (train_df["resp_3"] > 0) &
                      (train_df["resp_4"] > 0)).astype("int")

# Verify the new feature
print(train_df[["resp", "resp_1", "resp_2", "resp_3", "resp_4", "action"]].head())
```

### Verification

The output should display the first few rows of the DataFrame, including the new `action` feature, allowing you to verify that it has been correctly calculated.

#### Example Output

```plaintext
       resp  resp_1  resp_2  resp_3  resp_4  action
0  0.006270  0.009916  0.014079  0.008773  0.001390       1
1 -0.009792 -0.002828 -0.003226 -0.007319 -0.011114       0
2  0.023970  0.025134  0.027607  0.033406  0.034380       1
3 -0.003200 -0.004730 -0.003273 -0.000461 -0.000476       0
4 -0.002604  0.001252  0.002165 -0.001215 -0.006219       0
```

This output indicates that the `action` feature has been correctly calculated, signaling `1` for positive returns across all columns and `0` otherwise.
"""

# Define new feature named action , which can help investor to make decidion.

train_df["action"] = ((train_df["resp"] > 0) & (train_df["resp_1"] >0) & (train_df["resp_2"]>0)\
                     & (train_df["resp_3"]>0) & (train_df["resp_4"]>0)).astype("int")
#Y = np.stack((train_df[c]>0).astype("int") for c in resp_cols).T

"""<h2 id=feat_exploration> Features dataset explorations :

### Feature Dataset Exploration

In this section, we will explore the features dataset to understand the structure and characteristics of the features used in our model.

#### Steps Breakdown

1. **Loading the Features Data**:
   ```python
   # Load the features data
   features = pd.read_csv(path + "features.csv")
   ```

   - The `features.csv` file contains metadata about the features used in the dataset.
   - The `path` variable should be defined earlier to specify the directory where the `features.csv` file is located.

2. **Inspecting the Features Data**:
   ```python
   # Take a look at the features data
   features.head()
   ```

   - `features.head()`: Displays the first few rows of the features dataset to get an initial understanding of its structure.

### Example Data

```plaintext
    feature   tag_0   tag_1   tag_2   tag_3   tag_4   tag_5   tag_6   tag_7   tag_8   ...   tag_19   tag_20   tag_21   tag_22   tag_23   tag_24   tag_25   tag_26   tag_27   tag_28
0  feature_0  False  False  False  False  False  False  False  False  False  ...  False  False  False  False  False  False  False  False  False  False
1  feature_1  False  False  False  False  False  False  True   True   False  ...  False  False  False  False  False  False  False  False  False  False
2  feature_2  False  False  False  False  False  False  True   True   False  ...  False  False  False  False  False  False  False  False  False  False
3  feature_3  False  False  False  False  False  False  True   False  True   ...  False  False  False  False  False  False  False  False  False  False
4  feature_4  False  False  False  False  False  False  True   False  True   ...  False  False  False  False  False  False  False  False  False  False
5 rows × 30 columns
```

### Explanation of the Features Data

- **Columns**:
  - **feature**: The name of the feature (e.g., `feature_0`, `feature_1`, etc.).
  - **tag_0 to tag_28**: Binary tags (True/False) indicating the presence or absence of specific attributes or characteristics for each feature.

- **Rows**:
  - Each row corresponds to a different feature and shows which tags are applicable to that feature.

### Importance in Machine Learning Pipeline

- **Feature Tags**: The tags provide additional metadata that can be useful for feature selection and engineering.
- **Data Understanding**: Understanding the attributes and characteristics of each feature helps in designing better models and preprocessing steps.
- **Exploratory Data Analysis (EDA)**: Initial exploration of the features dataset is a critical step in EDA, providing insights that guide subsequent analyses and model development.

### Complete Code Context

Here's the complete context of the code for loading and inspecting the features dataset:

```python
import pandas as pd

# Define the path to the dataset
path = "path_to_your_dataset_directory/"

# Load the features data
features = pd.read_csv(path + "features.csv")

# Take a look at the features data
features.head()
```

### Summary

Exploring the features dataset provides valuable insights into the attributes and characteristics of the features used in the model. This understanding is crucial for effective feature engineering, selection, and model development, ultimately leading to more accurate and robust predictive models.

"""

# Load the features data.
features = pd.read_csv(path + "features.csv")

# Let's take a look at the features data
features.head()

"""### Statistical Overview of the Features Dataset

The `features` DataFrame contains metadata about the features used in the model, including various binary tags. Below is a summary of the structure and content of the dataset.

### Interpretation of the Output

1. **Rows and Columns**:
   - **Rows**: 130 rows, indicating there are 130 features in the dataset.
   - **Columns**: 30 columns, including the feature name and 29 binary tags.

2. **Column Details**:
   - **feature**: Contains the names of the features (e.g., `feature_0`, `feature_1`).
   - **tag_0 to tag_28**: Binary tags (True/False) indicating specific attributes or characteristics for each feature.

3. **Data Types**:
   - **bool**: 29 columns are of boolean type, representing the binary tags.
   - **object**: 1 column is of object type, representing the feature names.

4. **Non-Null Count**:
   - All columns have 130 non-null values, indicating there are no missing values in the dataset.

5. **Memory Usage**:
   - The memory usage of the DataFrame is 4.8 KB, which is efficient given the boolean data type for most columns.

### Importance in Machine Learning Pipeline

- **Data Integrity**: Ensuring there are no missing values in the metadata is crucial for consistent data processing.
- **Feature Selection**: The tags can be used for selecting or engineering new features based on their attributes.
- **Data Understanding**: Understanding the structure and content of the features dataset helps in designing better models and preprocessing steps.

### Example Use Case

With the `features` DataFrame loaded and understood, you can use the tags to filter or group features based on specific characteristics, aiding in feature engineering and model building.

### Complete Code Context

Here's the complete context of the code for loading, inspecting, and summarizing the features dataset:

```python
import pandas as pd

# Define the path to the dataset
path = "path_to_your_dataset_directory/"

# Load the features data
features = pd.read_csv(path + "features.csv")

# Display some statistics on the data
features.info()
```

### Summary

The `features` DataFrame provides a comprehensive overview of the features used in the model, including various binary tags that indicate specific attributes. Understanding this metadata is essential for effective feature engineering, selection, and model development, ultimately leading to more accurate and robust predictive models.
"""

# some statistics on the datas.
features.info()

"""**We can notice , that fortunatelly the datas don't have any missing values.**

### Dimensionality Reduction Using t-SNE

In this section, we apply t-SNE (t-distributed Stochastic Neighbor Embedding) to reduce the dimensionality of the features dataset and visualize it in two dimensions. This helps in understanding the structure and relationships within the high-dimensional data.

#### Key Steps

1. **Preprocessing**:
   - Changing the data type of the features DataFrame to `int8` for memory efficiency.
   - Setting the `feature` column as the index.

2. **t-SNE Implementation**:
   - Reducing the dataset to two dimensions using the t-SNE technique.

3. **Visualization**:
   - Plotting the 2D embedding of the features data using a scatter plot.

#### Code Breakdown

1. **Change Data Types and Set Index**:
   ```python
   # Change the type of features dataframe to int
   features.set_index('feature', inplace=True)
   features = features.astype("int8")
   ```

   - **`set_index('feature')`**: Sets the `feature` column as the index of the DataFrame.
   - **`astype("int8")`**: Converts all values in the DataFrame to `int8` type to save memory.

2. **t-SNE Implementation**:
   ```python
   from sklearn.manifold import TSNE

   # Apply t-SNE to reduce the dimensionality to 2 components
   t_sne = TSNE(n_components=2, random_state=42).fit_transform(features.values)
   ```

   - **`TSNE(n_components=2, random_state=42)`**: Initializes t-SNE to reduce the data to 2 dimensions with a fixed random state for reproducibility.
   - **`fit_transform(features.values)`**: Applies t-SNE to the values of the features DataFrame.

3. **Visualization**:
   ```python
   # Plotting the embedding of features data in two dimensions using t-SNE
   fig, ax = plt.subplots(1, 1, figsize=(10, 6))
   ax.scatter(t_sne[:, 0], t_sne[:, 1], cmap="coolwarm")
   ax.grid(True)
   ax.set_title("t-SNE")
   fig.suptitle("Dimensionality Reduction using t-SNE technique")
   plt.show()
   ```

   - **`fig, ax = plt.subplots(1, 1, figsize=(10, 6))`**: Creates a single subplot with a specified size.
   - **`ax.scatter(t_sne[:, 0], t_sne[:, 1], cmap="coolwarm")`**: Plots the 2D t-SNE embedding using a scatter plot.
   - **`ax.grid(True)`**: Adds a grid to the plot for better readability.
   - **`ax.set_title("t-SNE")`**: Sets the title of the plot.
   - **`fig.suptitle("Dimensionality Reduction using t-SNE technique")`**: Sets the main title of the figure.

### Output Interpretation

- **Scatter Plot**:
  - Each point in the scatter plot represents a feature in the reduced 2D space.
  - Points that are closer together in the plot indicate that the corresponding features are more similar in the high-dimensional space.

- **Grid and Titles**:
  - The grid helps in visualizing the distribution of points more clearly.
  - Titles provide context about the technique and the visualization.

#### Example Plot

![t-SNE Plot](image_url)

- **Clusters**: The scatter plot might show clusters of points, indicating groups of features that are similar to each other.
- **Separation**: Well-separated clusters suggest distinct groups of features with different characteristics.

### Summary

Using t-SNE for dimensionality reduction allows us to visualize high-dimensional data in a 2D space, providing insights into the structure and relationships within the data. This visualization can aid in feature selection, engineering, and understanding the underlying patterns in the dataset.

### Complete Code

Here's the complete code for preprocessing, applying t-SNE, and visualizing the features data:

```python
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.manifold import TSNE

# Load the features data
path = "path_to_your_dataset_directory/"
features = pd.read_csv(path + "features.csv")

# Change the type of features dataframe to int
features.set_index('feature', inplace=True)
features = features.astype("int8")

# Apply t-SNE to reduce the dimensionality to 2 components
t_sne = TSNE(n_components=2, random_state=42).fit_transform(features.values)

# Plotting the embedding of features data in two dimensions using t-SNE
fig, ax = plt.subplots(1, 1, figsize=(10, 6))
ax.scatter(t_sne[:, 0], t_sne[:, 1], cmap="coolwarm")
ax.grid(True)
ax.set_title("t-SNE")
fig.suptitle("Dimensionality Reduction using t-SNE technique")
plt.show()
```

This code will generate a scatter plot of the t-SNE embedding, helping to visualize the high-dimensional features data in two dimensions.
"""

# Change the type of features dataframe to int.
features.set_index('feature',inplace=True)
features = features.astype("int8")

#T_sne implementation
t_sne = tsne(n_components=2,random_state=42).fit_transform(features.values)

# Plotting the embedding of features datas in two dimension using the technique of TSNE.
fig,ax = plt.subplots(1,1,figsize=(10,6))
plt.scatter(t_sne[:,0],t_sne[:,1],cmap="coolwarm")
plt.grid(True)
plt.title("t_SNE")
plt.suptitle("Dimmensionality Reduction using TSNE technique")

"""==> The chart of Dimmensionality Reduction above , shows that the datas can be divided into some number of clusters. Indeed, we notice obviously, many accumulation of points, which are distant and they can be considered as a separated clusters.

### Selecting the Optimal Number of Clusters Using Silhouette Coefficient

In this section, we determine the best number of clusters for k-means clustering based on the silhouette coefficient. This method helps in identifying the optimal clustering that balances intra-cluster cohesion and inter-cluster separation.

#### Key Steps

1. **Define Cluster Numbers**:
   - Define a range of cluster numbers to test.
   
2. **K-means Clustering**:
   - Apply k-means clustering with different numbers of clusters.
   
3. **Calculate Silhouette Scores**:
   - Compute the silhouette score for each clustering result to evaluate its quality.
   
4. **Select the Best Number of Clusters**:
   - Identify the cluster number with the highest silhouette score.

#### Code Implementation

1. **Define Cluster Numbers and Initialize Dictionary**:
   ```python
   clusters_number = [4, 8, 12, 16, 20, 24, 28]  # Number of clusters to test in order to choose the best one
   silhouette_performances = {}  # Dictionary to store performance
   ```

2. **K-means Clustering and Silhouette Score Calculation**:
   ```python
   from sklearn.cluster import KMeans
   from sklearn.metrics import silhouette_score

   for cl_n in clusters_number:
       kmeans = KMeans(n_clusters=cl_n, random_state=42)
       kmeans.fit(features.values)
       sc = silhouette_score(features.values, kmeans.labels_)
       silhouette_performances[sc] = cl_n
   ```

   - **`KMeans(n_clusters=cl_n, random_state=42)`**: Initializes k-means clustering with a specified number of clusters and a fixed random state for reproducibility.
   - **`fit(features.values)`**: Fits the k-means model to the features data.
   - **`silhouette_score(features.values, kmeans.labels_)`**: Computes the silhouette score for the clustering result.
   - **`silhouette_performances[sc] = cl_n`**: Stores the silhouette score and the corresponding number of clusters in a dictionary.

3. **Visualize Silhouette Scores**:
   ```python
   import matplotlib.pyplot as plt
   import numpy as np

   # Plot the cluster performances in function of the number of clusters
   fig, ax = plt.subplots(1, 1, figsize=(15, 8))
   plt.plot(list(silhouette_performances.values()), list(silhouette_performances.keys()), marker='o')
   plt.title("Silhouette Score")
   plt.xlabel("Clusters Number", fontsize=10)
   plt.ylabel("Silhouette Score", fontsize=10)
   plt.ylim([0.15, 0.28])
   best_performance = np.max(list(silhouette_performances.keys()))
   best_clusters = silhouette_performances[best_performance]
   text = "best performance"
   plt.annotate(text, xy=(best_clusters, best_performance), arrowprops=dict(facecolor='black', shrink=0.05),
                xytext=(best_clusters + 2, best_performance + 0.005))
   plt.tick_params(axis="x", labelsize=15)
   plt.tick_params(axis="y", labelsize=15)
   plt.grid(True)
   plt.show()
   ```

   - **`plt.plot`**: Plots the silhouette scores against the number of clusters.
   - **`plt.annotate`**: Annotates the point with the best performance on the plot.

### Output Interpretation

- **Plot**: The x-axis represents the number of clusters, and the y-axis represents the silhouette score.
  - The plot shows the silhouette scores for different cluster numbers, helping to identify the optimal number of clusters.
- **Best Performance**: The annotation highlights the number of clusters with the highest silhouette score, indicating the best clustering performance.

#### Example Output

```plaintext
The best number of clusters is 24 with a silhouette score of 0.265
```

### Summary

Using the silhouette coefficient to evaluate different cluster numbers helps identify the optimal number of clusters for k-means clustering. This approach ensures that the clustering result has high quality, providing meaningful groupings of the features data.

### Complete Code

Here's the complete code for selecting the optimal number of clusters and visualizing the silhouette scores:

```python
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score

# Load the features data
path = "path_to_your_dataset_directory/"
features = pd.read_csv(path + "features.csv")

# Change the type of features dataframe to int
features.set_index('feature', inplace=True)
features = features.astype("int8")

# Define the number of clusters to test
clusters_number = [4, 8, 12, 16, 20, 24, 28]  # Number of clusters to test in order to choose the best one
silhouette_performances = {}  # Dictionary to store performance

# Evaluate the silhouette score for each number of clusters
for cl_n in clusters_number:
    kmeans = KMeans(n_clusters=cl_n, random_state=42)
    kmeans.fit(features.values)
    sc = silhouette_score(features.values, kmeans.labels_)
    silhouette_performances[sc] = cl_n

# Plot the cluster performances in function of the number of clusters
fig, ax = plt.subplots(1, 1, figsize=(15, 8))
plt.plot(list(silhouette_performances.values()), list(silhouette_performances.keys()), marker='o')
plt.title("Silhouette Score")
plt.xlabel("Clusters Number", fontsize=10)
plt.ylabel("Silhouette Score", fontsize=10)
plt.ylim([0.15, 0.28])
best_performance = np.max(list(silhouette_performances.keys()))
best_clusters = silhouette_performances[best_performance]
text = "best performance"
plt.annotate(text, xy=(best_clusters, best_performance), arrowprops=dict(facecolor='black', shrink=0.05),
             xytext=(best_clusters + 2, best_performance + 0.005))
plt.tick_params(axis="x", labelsize=15)
plt.tick_params(axis="y", labelsize=15)
plt.grid(True)
plt.show()

# Print the best number of clusters and the corresponding silhouette score
print(f"The best number of clusters is {best_clusters} with a silhouette score of {best_performance:.4f}")
```

This code helps you identify the optimal number of clusters for your features data, ensuring the best clustering performance based on the silhouette score.
"""

# We will choice the best number of cluster, who can give best performance using
# silhoute coefficient.

clusters_number = [4,8,12,16,20,24,28] # number of clusters to test in order to choice the best one.
silhouette_performances = {} # Dictionnary of performance.

for cl_n in clusters_number :
    kmeans = KMeans(n_clusters=cl_n)
    kmeans.fit(features.values)
    sc=silhouette_score(features.values,kmeans.labels_)
    silhouette_performances[sc] = cl_n

# plot the the cluster performances in function of the number of clusters.
fig,ax = plt.subplots(1,1,figsize=(15,8))
plt.plot(list(silhouette_performances.values()),list(silhouette_performances.keys()))
plt.title("Silhouette score ")
plt.xlabel("Clusters number",fontsize=10)
plt.ylabel("Silhouette score ",fontsize=10)
plt.ylim([0.15,0.28])
best_performace = np.max(list(silhouette_performances.keys()))
ab = silhouette_performances[best_performace]
text = "best performance"
plt.annotate(text,xy=(ab,best_performace),arrowprops=dict(facecolor='black', shrink=0.05),\
            xytext=(ab+2,best_performace + 0.005))
plt.tick_params(axis="x",labelsize=15)
plt.tick_params(axis="y",labelsize=15)

"""Let's now visualize theses defined clusters in two dimension, using T_SNE"""

best_model = KMeans(n_clusters=20)
best_model.fit(features.values)

"""### Visualizing Clusters Using t-SNE

In this section, we visualize the clusters formed by k-means clustering using t-SNE for dimensionality reduction. This helps in understanding the distribution and separation of clusters in the high-dimensional feature space.

#### Key Steps

1. **Apply t-SNE for Dimensionality Reduction**:
   - Reduce the high-dimensional data to two dimensions using t-SNE.
   
2. **Fit K-means Clustering**:
   - Fit the k-means model with the optimal number of clusters identified earlier.
   
3. **Plot the Clusters**:
   - Visualize the clusters in the 2D space obtained from t-SNE.

#### Code Implementation

1. **Apply t-SNE**:
   ```python
   from sklearn.manifold import TSNE

   # Apply t-SNE to reduce the dimensionality to 2 components
   t_sne = TSNE(n_components=2, random_state=42).fit_transform(features.values)
   ```

2. **Fit K-means Clustering**:
   ```python
   best_number_of_clusters = 24  # From previous silhouette score analysis
   best_model = KMeans(n_clusters=best_number_of_clusters, random_state=42)
   best_model.fit(features.values)
   ```

3. **Plot the Clusters**:
   ```python
   fig, ax = plt.subplots(1, 1, figsize=(15, 8))
   plt.scatter(t_sne[:, 0], t_sne[:, 1], c=best_model.labels_, cmap='viridis')
   plt.suptitle("Visualize Clusters Using t-SNE")
   plt.title("t-SNE")
   plt.grid(True)
   plt.show()
   ```

   - **`plt.scatter(t_sne[:, 0], t_sne[:, 1], c=best_model.labels_, cmap='viridis')`**: Plots the t-SNE embedding, coloring the points according to their cluster labels from the k-means model.
   - **`plt.suptitle("Visualize Clusters Using t-SNE")`**: Sets the main title of the plot.
   - **`plt.title("t-SNE")`**: Sets the subtitle of the plot.
   - **`plt.grid(True)`**: Adds a grid to the plot for better readability.

### Output Interpretation

- **Scatter Plot**: The plot displays the 2D embedding of the high-dimensional data, with points colored according to their cluster assignments.
  - Each color represents a different cluster.
  - The distribution and separation of clusters in the plot indicate how well the clustering algorithm has grouped similar features together.

### Complete Code

Here's the complete code for applying t-SNE, fitting k-means clustering, and visualizing the clusters:

```python
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.manifold import TSNE

# Load the features data
path = "path_to_your_dataset_directory/"
features = pd.read_csv(path + "features.csv")

# Change the type of features dataframe to int
features.set_index('feature', inplace=True)
features = features.astype("int8")

# Apply t-SNE to reduce the dimensionality to 2 components
t_sne = TSNE(n_components=2, random_state=42).fit_transform(features.values)

# Fit the k-means model with the optimal number of clusters
best_number_of_clusters = 24  # From previous silhouette score analysis
best_model = KMeans(n_clusters=best_number_of_clusters, random_state=42)
best_model.fit(features.values)

# Plotting the clusters
fig, ax = plt.subplots(1, 1, figsize=(15, 8))
plt.scatter(t_sne[:, 0], t_sne[:, 1], c=best_model.labels_, cmap='viridis')
plt.suptitle("Visualize Clusters Using t-SNE")
plt.title("t-SNE")
plt.grid(True)
plt.show()
```

### Summary

By visualizing the clusters using t-SNE, we can see how well the k-means algorithm has grouped similar features together in the high-dimensional space. This visualization helps in understanding the structure and distribution of clusters, providing insights into the underlying patterns in the data.
"""

fig,ax = plt.subplots(1,1,figsize=(15,8))
plt.scatter(t_sne[:,0],t_sne[:,1],c=best_model.labels_)
plt.suptitle("Visualize clusters using T_SNE")
plt.title("T_SNE")
plt.grid(True)

"""<h2 id=modeling> Modeling :</h2>

### Modeling

In this section, we prepare the dataset for modeling by filtering and cleaning the data, removing unnecessary features, and visualizing the class distribution.

#### Key Steps

1. **Filtering the Data**:
   - Remove rows with `date` less than or equal to 85 to focus on recent data.
   
2. **Feature Selection**:
   - Remove columns that are not needed for the model.
   
3. **Save Feature Names**:
   - Save the names of retained features for future reference.
   
4. **Visualize Class Imbalance**:
   - Plot a pie chart to show the distribution of the target variable.

#### Code Implementation

1. **Filter the Data**:
   ```python
   train_df = train_df.query('date > 85').reset_index(drop=True)
   ```

   - **`query('date > 85')`**: Filters the dataset to keep only rows with `date` greater than 85.
   - **`reset_index(drop=True)`**: Resets the index of the filtered DataFrame.

2. **Feature Selection**:
   ```python
   features = list(train_df.columns)  # List of retained features
   features_to_remove = ["weight", "resp_1", "resp_2", "resp_3", "resp_4", "resp", "action", "ts_id", "date", "feature_0"]
   
   for feature in features_to_remove:
       features.remove(feature)
   ```

   - **`features_to_remove`**: List of columns to be removed from the features list.
   - **`for feature in features_to_remove: features.remove(feature)`**: Removes the specified columns from the features list.

3. **Save Feature Names**:
   ```python
   import pickle

   def save_pickle(dict_param, nom_fich):
       with open(nom_fich, "wb") as f:
           pickle.dump(dict_param, f)

   save_pickle(features, "features_names")
   ```

   - **`save_pickle(features, "features_names")`**: Saves the list of retained feature names to a file using pickle.

4. **Visualize Class Imbalance**:
   ```python
   import plotly.express as px

   fig = px.pie(train_df.loc[train_df["weight"] > 0, :], names="action", title="Class Imbalance")
   fig.update_layout(title={"x": 0.475, "y": 0.9, "xanchor": "center", "yanchor": "top"})
   fig.show()
   ```

   - **`train_df.loc[train_df["weight"] > 0, :]`**: Filters the dataset to include only rows with `weight` greater than 0.
   - **`px.pie(names="action", title="Class Imbalance")`**: Creates a pie chart to show the distribution of the `action` column.
   - **`fig.update_layout(title={"x": 0.475, "y": 0.9, "xanchor": "center", "yanchor": "top"})`**: Adjusts the title position of the plot.

### Summary

By filtering the dataset, selecting relevant features, and visualizing the class distribution, we prepare the data for modeling. This ensures that the model is trained on the most recent and relevant data, and it helps in understanding the class distribution which is crucial for model evaluation and performance metrics.

### Complete Code

Here's the complete code for data preparation and class imbalance visualization:

```python
import pandas as pd
import pickle
import plotly.express as px

# Load the train data
path = "path_to_your_dataset_directory/"
train_df = pd.read_csv(path + "train.csv")

# Filter the data
train_df = train_df.query('date > 85').reset_index(drop=True)

# Feature selection
features = list(train_df.columns)  # List of retained features
features_to_remove = ["weight", "resp_1", "resp_2", "resp_3", "resp_4", "resp", "action", "ts_id", "date", "feature_0"]

for feature in features_to_remove:
    features.remove(feature)

# Save feature names
def save_pickle(dict_param, nom_fich):
    with open(nom_fich, "wb") as f:
        pickle.dump(dict_param, f)

save_pickle(features, "features_names")

# Visualize class imbalance
fig = px.pie(train_df.loc[train_df["weight"] > 0, :], names="action", title="Class Imbalance")
fig.update_layout(title={"x": 0.475, "y": 0.9, "xanchor": "center", "yanchor": "top"})
fig.show()
```

This code prepares the data for modeling by filtering and selecting relevant features and visualizes the class distribution to understand the balance between different classes in the target variable.
"""

train_df = train_df.query('date > 85').reset_index(drop = True)

features = list(train_df.columns) # list of retaind features
features.remove("weight")
features.remove("resp_1")
features.remove("resp_2")
features.remove("resp_3")
features.remove("resp_4")
features.remove("resp")
features.remove("action")
features.remove("ts_id")
features.remove("date")
features.remove("feature_0")

save_pickle(features,"features_names")

fig=px.pie(train_df.loc[train_df["weight"] > 0,:],names="action",title="Class imballance")
fig.update_layout(title={"x":0.475,"y":0.9,"xanchor":"center","yanchor":"top"})

"""==> The chart above , show that there is a class imballance , that we should correctly tackled in order to not biased the performance of our model.

In order to tackle correctly the class imbalnce problem, the easiest way to succefully generalise is to use more datas.The problem is that out-of-the-box classifiers like logistic regression or random forest tend to generalize by discarding the rare class. One easy best practice is building n models that use all the samples of the rare class and n-differing samples of the abundant class. Given that you want to ensemble 10 models, you would keep e.g. the 1.000 cases of the rare class and randomly sample 10.000 cases of the abundant class. Then you just split the 10.000 cases in 10 chunks and train 10 different models.</font>

![](https://www.kdnuggets.com/wp-content/uploads/imbalanced-data-2.png)

In our case, the size of the abundant class is three time as bigger as the rarely class. So we need to train three models , by splitting the datas of the abundant class to three sample , and use the the datas of the rare class for each model training.

### Data Balancing and Splitting for Model Training

In this section, we handle class imbalance by balancing the dataset and creating multiple balanced chunks of data for model training. This approach ensures that our models are trained on balanced data, which helps in improving model performance and reducing bias towards the abundant class.

#### Key Steps

1. **Extract Abundant and Rare Classes**:
   - Separate the dataset into abundant and rare classes based on the target variable `action`.
   
2. **Calculate Mean Imputer Values**:
   - Compute the mean values of features for imputing missing values in real production scenarios.
   
3. **Shuffle and Split Data**:
   - Shuffle the data and split the abundant class data into chunks, then append the rare class data to each chunk.
   
4. **Save Imputer Values**:
   - Save the mean imputer values for future use.

#### Code Implementation

1. **Extract Abundant and Rare Classes**:
   ```python
   abundant_class = train_df.loc[(train_df["weight"] > 0) & (train_df["action"] == 0), :]  # Extract abundant data
   rare_class = train_df.loc[(train_df["weight"] > 0) & (train_df["action"] == 1), :]    # Extract rare data
   ```

2. **Calculate Mean Imputer Values**:
   ```python
   import numpy as np
   import pickle

   # Calculate mean values for imputation
   imputer = np.mean(train_df[features].values, axis=0)

   # Save the imputer values
   def save_pickle(dict_param, nom_fich):
       with open(nom_fich, "wb") as f:
           pickle.dump(dict_param, f)

   save_pickle(imputer, "features_imputation")
   ```

3. **Shuffle and Split Data**:
   ```python
   # Shuffle the data
   abundant_class = abundant_class.sample(frac=1)
   rare_class = rare_class.sample(frac=1)

   # Calculate the size of the rare class
   l = len(rare_class)

   # Create balanced data chunks
   df1 = abundant_class.iloc[:l, :].append(rare_class)  # 1st chunk
   df2 = abundant_class.iloc[l:2*l, :].append(rare_class)  # 2nd chunk
   df3 = abundant_class.iloc[2*l:, :].append(rare_class)  # 3rd chunk

   # Shuffle the data chunks
   df1 = df1.sample(frac=1)
   df2 = df2.sample(frac=1)
   df3 = df3.sample(frac=1)

   # Print the sizes of the data chunks
   len(df1), len(df2), len(df3)
   ```

   - **Shuffle**: The `sample(frac=1)` method shuffles the data.
   - **Split and Append**: The abundant class data is split into chunks, and the rare class data is appended to each chunk to create balanced datasets.
   - **Shuffle Again**: The balanced data chunks are shuffled again to ensure randomness.

### Output

```plaintext
(832062, 832062, 739353)
```

- The output shows the sizes of the three balanced data chunks created for model training.

### Summary

By handling class imbalance through balancing and splitting the dataset into chunks, we ensure that our models are trained on data that represents both classes equally. This approach helps in improving model performance and fairness.

### Complete Code

Here's the complete code for balancing the dataset, calculating imputer values, and creating balanced data chunks for model training:

```python
import pandas as pd
import numpy as np
import pickle

# Load the train data
path = "path_to_your_dataset_directory/"
train_df = pd.read_csv(path + "train.csv")

# Filter the data
train_df = train_df.query('date > 85').reset_index(drop=True)

# Feature selection
features = list(train_df.columns)  # List of retained features
features_to_remove = ["weight", "resp_1", "resp_2", "resp_3", "resp_4", "resp", "action", "ts_id", "date", "feature_0"]

for feature in features_to_remove:
    features.remove(feature)

# Calculate mean values for imputation
imputer = np.mean(train_df[features].values, axis=0)

# Save the imputer values
def save_pickle(dict_param, nom_fich):
    with open(nom_fich, "wb") as f:
        pickle.dump(dict_param, f)

save_pickle(imputer, "features_imputation")

# Extract abundant and rare classes
abundant_class = train_df.loc[(train_df["weight"] > 0) & (train_df["action"] == 0), :]  # Extract abundant data
rare_class = train_df.loc[(train_df["weight"] > 0) & (train_df["action"] == 1), :]    # Extract rare data

# Shuffle the data
abundant_class = abundant_class.sample(frac=1)
rare_class = rare_class.sample(frac=1)

# Calculate the size of the rare class
l = len(rare_class)

# Create balanced data chunks
df1 = abundant_class.iloc[:l, :].append(rare_class)  # 1st chunk
df2 = abundant_class.iloc[l:2*l, :].append(rare_class)  # 2nd chunk
df3 = abundant_class.iloc[2*l:, :].append(rare_class)  # 3rd chunk

# Shuffle the data chunks
df1 = df1.sample(frac=1)
df2 = df2.sample(frac=1)
df3 = df3.sample(frac=1)

# Print the sizes of the data chunks
print(len(df1), len(df2), len(df3))
```

This code balances the dataset and prepares multiple data chunks for model training, ensuring that the models are trained on balanced data, which is crucial for robust and fair performance.
"""

abundant_class = train_df.loc[(train_df["weight"] > 0) & (train_df["action"]==0),:] # extract datas which concern abundant datas
rare_class = train_df.loc[(train_df["weight"] > 0)&(train_df["action"]==1),:]   # extract datas which concern rares datas.

# The mean values of each feature to use in order to impute missing values in real production.
imputer = np.mean(train_df[features].values,axis=0)
save_pickle(imputer,"features_imputation")

abundant_class = abundant_class.sample(frac=1)
rare_class = rare_class.sample(frac=1)

l = len(rare_class) # the size of data which concern rare class.
df1 = abundant_class.iloc[:l,:].append(rare_class) # 1st chunk of datas to train first model
df2 = abundant_class.iloc[l:2*l,:].append(rare_class) # 2nd chunk of datas to train second model
df3 = abundant_class.iloc[2*l:,:].append(rare_class) # 3nd chunk of datas to train the third model.

df1 = df1.sample(frac=1) # shuffle the datas
df2 = df2.sample(frac=1) # shuffle the datas
df3 = df3.sample(frac=1) # shuffle the datas

len(df1),len(df2),len(df3)

"""### Explanation of the Code Snippet

1. **Memory Management**:
   - **Delete Unused Data**:
     ```python
     del(train_df)
     del(train)
     ```
     - Frees up memory by deleting the `train_df` and `train` DataFrames.

2. **Prepare Training Data**:
   - **Retain Features and Labels**:
     ```python
     training = [df1[features], df2[features], df3[features]]
     targets = [df1["action"], df2["action"], df3["action"]]
     ```
     - Creates lists of features and labels for each balanced data chunk (`df1`, `df2`, `df3`).

3. **Split Data for Training and Validation**:
   - **Train-Test Split**:
     ```python
     datas = []
     for i in range(3):
         xtr, xval, ytr, yval = train_test_split(training[i].values, targets[i].values, test_size=0.1, stratify=targets[i].values)
         datas.append(((xtr, ytr), (xval, yval)))
     ```
     - Splits each data chunk into training and validation sets (90% training, 10% validation) while preserving class distribution (`stratify`).

4. **Memory Cleanup**:
   - **Delete Temporary Data and Run Garbage Collection**:
     ```python
     del(df1)
     del(df2)
     del(df3)
     del(abundant_class)
     del(rare_class)
     del(features_with_missing_values)
     del(correlation_table)
     del(missing_table)

     gc.collect()
     ```
     - Deletes intermediate variables and runs garbage collection to free up memory, resulting in `783067` bytes of memory freed.

### Importance in the Machine Learning Pipeline

- **Memory Management**: Essential for handling large datasets efficiently.
- **Data Preparation**: Ensures that training and validation datasets are balanced and ready for model training.
- **Stratified Splitting**: Maintains class distribution, crucial for training robust models.
- **Garbage Collection**: Optimizes memory usage, preventing potential memory overflow issues.

This step is crucial for preparing clean, balanced datasets for training multiple models while efficiently managing memory resources.
"""

# reduce the memory charge
del(train_df)
del(train)

# retained features and label to train differents models on differents chunk of datas.
training= [df1[features],df2[features],df3[features]]
targets = [df1["action"],df2["action"],df3["action"]]

datas = []
for i in range(3):
    xtr,xval,ytr,yval = train_test_split(training[i].values,targets[i].values,test_size=0.1,\
                                        stratify=targets[i].values)
    datas.append(((xtr,ytr),(xval,yval)))

# refresh memory
del(df1)
del(df2)
del(df3)
del(abundant_class)
del(rare_class)
del(features_with_missing_values)
del(correlation_table)
del(missing_table)

gc.collect()

"""### Modeling Step

In this step, we train multiple models on different chunks of data using LightGBM and a custom neural network, and then save the trained models for future use.

#### Key Steps

1. **Set Hyperparameters**:
   - Define the parameters for the LightGBM model.

2. **Train LightGBM Models**:
   - Train and validate LightGBM models on different data chunks.
   - Save the trained models.

3. **Train Neural Network Models**:
   - Define and compile a custom neural network model using TensorFlow.
   - Train the neural network on different data chunks.

#### Code Implementation

1. **Set Hyperparameters**:
   ```python
   params = {
       "num_leaves": 300,
       "max_bin": 450,
       "feature_fraction": 0.52,
       "bagging_fraction": 0.52,
       "objective": "binary",
       "learning_rate": 0.05,
       "boosting_type": "gbdt",
       "metric": "auc"
   }
   ```

2. **Train LightGBM Models**:
   ```python
   import lightgbm as lgbm
   from sklearn.model_selection import train_test_split

   models = []  # List to store trained models

   for i in range(3):
       xtr, ytr = datas[i][0]
       xval, yval = datas[i][1]
       d_train = lgbm.Dataset(xtr, label=ytr)
       d_eval = lgbm.Dataset(xval, label=yval, reference=d_train)
       
       clf = lgbm.train(
           params,
           d_train,
           valid_sets=[d_train, d_eval],
           num_boost_round=1500,
           early_stopping_rounds=50,
           verbose_eval=50
       )
       clf.save_model(f"weights_{i}")
       models.append(clf)
   ```

3. **Train Neural Network Models**:
   ```python
   import tensorflow as tf
   from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization
   from tensorflow.keras.models import Model
   from tensorflow.keras.activations import swish
   from tensorflow.keras.losses import BinaryCrossentropy
   from tensorflow.keras.optimizers import Adam

   SEED = 1111

   def create_model(n_features, dr_rate, hidden_units, n_labels, label_smoothing, lr):
       inp = Input(shape=(n_features,))
       x = BatchNormalization()(inp)
       x = Dropout(dr_rate[0])(x)
       for i in range(len(hidden_units)):
           x = Dense(hidden_units[i])(x)
           x = BatchNormalization()(x)
           x = swish(x)
           x = Dropout(dr_rate[i+1])(x)
       out = Dense(n_labels, activation="sigmoid")(x)
       model = Model(inputs=inp, outputs=out)

       model.compile(
           loss=BinaryCrossentropy(label_smoothing=label_smoothing),
           optimizer=Adam(learning_rate=lr),
           metrics=[tf.keras.metrics.AUC(name="AUC")]
       )
       return model

   tf.random.set_seed(SEED)
   np.random.seed(SEED)

   models = []
   n_features = len(features)
   dr_rate = [0.2, 0.2, 0.2, 0.2]
   hidden_units = [150, 150, 150]
   label_smoothing = 1e-2
   lr = 1e-3
   batch_size = 5000

   for i in range(len(datas)):
       xtr, ytr = datas[i][0]
       xval, yval = datas[i][1]
       clf = create_model(n_features, dr_rate, hidden_units, 1, label_smoothing, lr)
       clf.fit(
           xtr, ytr,
           validation_data=(xval, yval),
           batch_size=batch_size,
           epochs=4
       )
       models.append(clf)
   ```

### Explanation

1. **Hyperparameters**:
   - **`num_leaves`, `max_bin`**: Parameters controlling the complexity of the LightGBM model.
   - **`feature_fraction`, `bagging_fraction`**: Parameters for feature and data sampling to prevent overfitting.
   - **`learning_rate`**: Step size for updating the model weights.
   - **`boosting_type`, `metric`**: Specifies the boosting method and evaluation metric.

2. **LightGBM Models**:
   - **Data Splitting**: Each data chunk is split into training and validation sets.
   - **Early Stopping**: Stops training if the validation score doesn't improve for 50 rounds.
   - **Model Saving**: Trained models are saved to disk.

3. **Neural Network Models**:
   - **Custom Model**: A neural network with batch normalization, dropout layers, and swish activation functions.
   - **Training**: The model is trained on each data chunk with early stopping based on validation performance.
   - **Metrics**: Uses the AUC metric to evaluate model performance.

### Summary

This step involves training multiple models using LightGBM and a custom neural network on different data chunks. The models are trained with early stopping to prevent overfitting and are saved for future use. This approach ensures robust model training and evaluation.
"""

# modeling step
params={"num_leaves":300,
       "max_bin":450,
       "feature_fraction":0.52,
       "bagging_fraction":0.52,
       "objective":"binary",
       "learning_rate":0.05,
       "boosting_type":"gbdt",
       "metric":"auc"
       }
#kf = KFold(n_splits=3,shuffle=True,random_state=111)
models = [] # list of model , we will train
for i in range(3):
    xtr = datas[i][0][0]
    ytr = datas[i][0][1]
    xval = datas[i][1][0]
    yval = datas[i][1][1]
    #xval = val_datas[j].loc[:,features]
    #yval = val_datas[j].loc[:,"action"]
    d_train = lgbm.Dataset(xtr,label=ytr)
    d_eval = lgbm.Dataset(xval,label=yval,reference=d_train)
    clf = lgbm.train(params,d_train,valid_sets=[d_train,d_eval],num_boost_round=1500,\
                    early_stopping_rounds=50,verbose_eval=50)
    clf.save_model("weights_{}".format(i))
    models.append(clf)

"""import tensorflow as tf
from tensorflow.keras.layers import Input,Dense, Dropout,BatchNormalization
from tensorflow.keras.models import Model
from tensorflow.keras.activations import swish
from tensorflow.keras.losses import BinaryCrossentropy
from tensorflow.keras.optimizers import Adam
SEED = 1111

def create_model(n_features,dr_rate,hidden_units,n_labels,label_smoothing,lr):
    inp = Input(shape=(n_features,))
    x = BatchNormalization()(inp)
    x = Dropout(dr_rate[0])(x)
    for i in range(len(hidden_units)):
        x = Dense(hidden_units[0])(x)
        x = BatchNormalization()(x)
        x = swish(x)
        x = Dropout(dr_rate[i+1])(x)
    out = Dense(n_labels,activation="sigmoid")(x)
    model = Model(inputs=inp,outputs=out)
    
    model.compile(loss = BinaryCrossentropy(label_smoothing=label_smoothing),\
                  optimizer = Adam(learning_rate=lr),metrics= tf.keras.metrics.AUC(name="AUC"))
    return model

tf.random.set_seed(SEED)
np.random.seed(SEED)
models = []
n_features = len(features)
dr_rate = [0.2,0.2,0.2,0.2]
hidden_units = [150,150,150]
label_smoothing = 1e-2
lr = 1e-3
batch_size = 5000
for i in range(len(datas)):
    hist = []
    j = (i+1)% 2
    clf = create_model(n_features,dr_rate,hidden_units,1,label_smoothing,lr)
    clf.fit(datas[i].values,target[i].values,validation_data = (datas[j].values,\
                                                                target[j].values),batch_size=batch_size,epochs=4)
    
    hist.append(clf)
    models.append(hist[-1])

### Feature Importance Visualization

In this section, we visualize the feature importance for each of the three LightGBM models. Feature importance helps in understanding which features have the most influence on the model's predictions.

#### Key Steps

1. **Plot Feature Importance**:
   - Generate feature importance plots for each of the trained models.

2. **Interpret the Plots**:
   - Identify the most important features for each model.

#### Code Implementation

1. **Plot Feature Importance**:
   ```python
   import matplotlib.pyplot as plt
   import lightgbm as lgbm

   fig, ax = plt.subplots(1, 3, figsize=(15, 25))  # Adjust the figsize for better visualization

   for i in range(3):
       lgbm.plot_importance(models[i], ax=ax[i], max_num_features=100, importance_type='split', title=f'Feature importance - Model {i+1}')
       ax[i].title.set_size(16)
       ax[i].tick_params(axis='y', labelsize=12)
       ax[i].tick_params(axis='x', labelsize=12)

   plt.tight_layout()
   plt.show()
   ```

   - **`fig, ax = plt.subplots(1, 3, figsize=(15, 25))`**: Creates a subplot with 3 columns and a specified figure size for better readability.
   - **`lgbm.plot_importance(models[i], ax=ax[i], max_num_features=100, importance_type='split', title=f'Feature importance - Model {i+1}')`**: Plots the top 100 features by importance for each model.
   - **`plt.tight_layout()`**: Adjusts the layout to prevent overlap of elements.

2. **Interpret the Plots**:
   - **Feature Importance**: The length of the bars indicates the importance of each feature. Features at the top of each plot have the most significant impact on the model's predictions.

### Summary

The feature importance plots provide insights into which features are most influential in the models' predictions. This information can be used to understand the model's behavior and to perform feature selection or engineering in future iterations.

### Complete Code

Here's the complete code for generating and displaying the feature importance plots for the LightGBM models:

```python
import matplotlib.pyplot as plt
import lightgbm as lgbm

# Assuming models is a list of trained LightGBM models
fig, ax = plt.subplots(1, 3, figsize=(15, 25))  # Adjust the figsize for better visualization

for i in range(3):
    lgbm.plot_importance(models[i], ax=ax[i], max_num_features=100, importance_type='split', title=f'Feature importance - Model {i+1}')
    ax[i].title.set_size(16)
    ax[i].tick_params(axis='y', labelsize=12)
    ax[i].tick_params(axis='x', labelsize=12)

plt.tight_layout()
plt.show()
```

This code will generate side-by-side plots of the top 100 most important features for each of the three LightGBM models, providing a clear visualization of which features are most influential in the models' predictions.
"""

fig,ax = plt.subplots(1,3,figsize=(10,20))
for i in range(3):
    lgbm.plot_importance(models[i],ax=ax[i])

"""<h2 id=submission> Submission </h2>

### Making Predictions in Real-Time

In this section, we implement the real-time prediction step using the Jane Street environment. The goal is to make predictions on the test data provided in real-time and submit these predictions.

#### Key Steps

1. **Initialize the Environment**:
   - Create the Jane Street environment for making predictions.
   
2. **Define Threshold for Predictions**:
   - Set a threshold for deciding whether to take action based on model predictions.
   
3. **Iterate Over Test Data**:
   - For each test sample, preprocess the data, make predictions using the trained models, and submit the predictions.

#### Code Implementation

1. **Initialize the Environment**:
   ```python
   import janestreet
   from tqdm import tqdm

   env = janestreet.make_env()  # Initialize the environment
   ```

2. **Define Threshold for Predictions**:
   ```python
   th = 0.5000  # Set the threshold for making predictions
   ```

3. **Iterate Over Test Data and Make Predictions**:
   ```python
   for (test_df, pred_df) in tqdm(env.iter_test()):
       if test_df["weight"].item() > 0:
           x_tt = test_df.loc[:, features].values
           if np.isnan(x_tt.sum()):
               x_tt = np.nan_to_num(x_tt) + np.isnan(x_tt) * imputer
           pred = np.mean(np.stack([model.predict(x_tt) for model in models]), axis=0).T
           pred_df.action = np.where(pred >= th, 1, 0).astype(int)
       else:
           pred_df.action = 0
       
       env.predict(pred_df)
   ```

   - **`test_df["weight"].item() > 0`**: Only make predictions for samples with a positive weight.
   - **`x_tt = test_df.loc[:, features].values`**: Extract feature values from the test sample.
   - **`np.nan_to_num(x_tt) + np.isnan(x_tt) * imputer`**: Replace NaN values with the imputed mean values.
   - **`pred = np.mean(np.stack([model.predict(x_tt) for model in models]), axis=0).T`**: Make predictions using all models and average the results.
   - **`pred_df.action = np.where(pred >= th, 1, 0).astype(int)`**: Apply the threshold to determine the action.
   - **`env.predict(pred_df)`**: Submit the prediction to the environment.

### Explanation

- **Environment Initialization**: The `janestreet.make_env()` function initializes the environment for making real-time predictions.
- **Threshold**: The `th` variable is set to 0.5, meaning that if the average prediction probability is 0.5 or higher, the action will be 1 (i.e., make the trade), otherwise it will be 0 (i.e., do not make the trade).
- **Iterating Over Test Data**: The `tqdm` library is used to display a progress bar as the model iterates over the test data.
  - **Preprocessing**: NaN values in the test data are replaced with the mean imputed values.
  - **Prediction**: Predictions from multiple models are averaged to determine the final prediction.
  - **Submission**: The predictions are submitted to the environment in real-time.

### Complete Code

Here's the complete code for making real-time predictions using the Jane Street environment:

```python
import janestreet
import numpy as np
from tqdm import tqdm

# Initialize the environment
env = janestreet.make_env()

# Set the prediction threshold
th = 0.5000

# Iterate over the test data
for (test_df, pred_df) in tqdm(env.iter_test()):
    if test_df["weight"].item() > 0:
        x_tt = test_df.loc[:, features].values
        if np.isnan(x_tt.sum()):
            x_tt = np.nan_to_num(x_tt) + np.isnan(x_tt) * imputer
        pred = np.mean(np.stack([model.predict(x_tt) for model in models]), axis=0).T
        pred_df.action = np.where(pred >= th, 1, 0).astype(int)
    else:
        pred_df.action = 0
    
    env.predict(pred_df)
```

### Summary

This step involves making real-time predictions using the trained models and the Jane Street environment. By preprocessing the test data, averaging predictions from multiple models, and applying a threshold, we determine whether to take action on each test sample and submit the predictions accordingly.
"""

import janestreet
env = janestreet.make_env()

th = 0.5000

for (test_df, pred_df) in tqdm(env.iter_test()):
    if test_df["weight"].item() > 0 :
        x_tt = test_df.loc[:, features].values
        if np.isnan(x_tt.sum()):
           x_tt = np.nan_to_num(x_tt) + np.isnan(x_tt) * imputer
        pred = np.mean(np.stack([model.predict(x_tt) for model in models]),axis=0).T
        pred_df.action = np.where(pred >= th, 1, 0).astype(int)
    else :
        pred_df.action = 0

    env.predict(pred_df)